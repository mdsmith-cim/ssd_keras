{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD300 Inference Tutorial\n",
    "\n",
    "This is a brief tutorial that shows how to use a trained SSD300 for inference on the Pascal VOC datasets. If you'd like more detailed explanations, please refer to [`ssd300_training.ipynb`](https://github.com/pierluigiferrari/ssd_keras/blob/master/ssd300_training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from scipy.stats import entropy\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import os.path as p\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Model\n",
    "from ssd_encoder_decoder.ssd_output_decoder_dropout import decode_detections\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from data_generator.object_detection_2d_image_boxes_validation_utils import BoxFilter\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "\n",
    "from bounding_box_utils.bounding_box_utils import iou\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image size.\n",
    "img_height = 300\n",
    "img_width = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a trained SSD\n",
    "\n",
    "Either load a trained model or build a model and load trained weights into it. Since the HDF5 files I'm providing contain only the weights for the various SSD versions, not the complete models, you'll have to go with the latter option when using this implementation for the first time. You can then of course save the model and next time load the full model directly, without having to build it.\n",
    "\n",
    "You can find the download links to all the trained model weights in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Build the model and load trained weights into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True\n",
    "n_boxes = 8732\n",
    "N = 20 # Number of passes through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1122 21:05:34.424905 140538163132160 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W1122 21:05:34.425534 140538163132160 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1122 21:05:34.437798 140538163132160 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1122 21:05:34.438585 140538163132160 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1122 21:05:34.448341 140538163132160 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W1122 21:05:34.473222 140538163132160 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1122 21:05:34.626232 140538163132160 deprecation.py:506] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1122 21:05:36.912755 140538163132160 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1122 21:05:36.936220 140538163132160 deprecation.py:323] From /usr/local/data/msmith/uncertainty/ssd_keras/keras_loss_function/keras_ssd_loss.py:133: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1122 21:05:36.950165 140538163132160 deprecation.py:323] From /usr/local/data/msmith/uncertainty/ssd_keras/keras_loss_function/keras_ssd_loss.py:74: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1122 21:05:36.968971 140538163132160 deprecation.py:323] From /usr/local/data/msmith/uncertainty/ssd_keras/keras_loss_function/keras_ssd_loss.py:166: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                swap_channels=swap_channels)\n",
    "\n",
    "\n",
    "# 2: Load the trained weights into the model.\n",
    "\n",
    "# TODO: Set the path of the trained weights.\n",
    "weights_path = 'good_dropout_model/ssd300_dropout_PASCAL2012_train_+12_epoch-58_loss-3.8960_val_loss-5.0832.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Compile the model so that Keras won't complain the next time you load it.\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300, 300, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "identity_layer (Lambda)         (None, 300, 300, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_mean_normalization (Lambd (None, 300, 300, 3)  0           identity_layer[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_channel_swap (Lambda)     (None, 300, 300, 3)  0           input_mean_normalization[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                (None, 300, 300, 64) 1792        input_channel_swap[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2 (Conv2D)                (None, 300, 300, 64) 36928       conv1_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 150, 150, 64) 0           conv1_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1 (Conv2D)                (None, 150, 150, 128 73856       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2 (Conv2D)                (None, 150, 150, 128 147584      conv2_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 75, 75, 128)  0           conv2_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1 (Conv2D)                (None, 75, 75, 256)  295168      pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2 (Conv2D)                (None, 75, 75, 256)  590080      conv3_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3 (Conv2D)                (None, 75, 75, 256)  590080      conv3_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 38, 38, 256)  0           conv3_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1 (Conv2D)                (None, 38, 38, 512)  1180160     pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling2D)            (None, 19, 19, 512)  0           conv4_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1 (Conv2D)                (None, 19, 19, 512)  2359808     pool4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 19, 19, 512)  0           conv5_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fc6 (Conv2D)                    (None, 19, 19, 1024) 4719616     pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 19, 19, 1024) 0           fc6[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "fc7 (Conv2D)                    (None, 19, 19, 1024) 1049600     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 19, 19, 1024) 0           fc7[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv6_1 (Conv2D)                (None, 19, 19, 256)  262400      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv6_padding (ZeroPadding2D)   (None, 21, 21, 256)  0           conv6_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2 (Conv2D)                (None, 10, 10, 512)  1180160     conv6_padding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv7_1 (Conv2D)                (None, 10, 10, 128)  65664       conv6_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_padding (ZeroPadding2D)   (None, 12, 12, 128)  0           conv7_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2 (Conv2D)                (None, 5, 5, 256)    295168      conv7_padding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv8_1 (Conv2D)                (None, 5, 5, 128)    32896       conv7_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2 (Conv2D)                (None, 3, 3, 256)    295168      conv8_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_1 (Conv2D)                (None, 3, 3, 128)    32896       conv8_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm (L2Normalization)  (None, 38, 38, 512)  512         conv4_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2 (Conv2D)                (None, 1, 1, 256)    295168      conv9_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf (Conv2D) (None, 38, 38, 84)   387156      conv4_3_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_conf (Conv2D)          (None, 19, 19, 126)  1161342     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf (Conv2D)      (None, 10, 10, 126)  580734      conv6_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf (Conv2D)      (None, 5, 5, 126)    290430      conv7_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf (Conv2D)      (None, 3, 3, 84)     193620      conv8_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf (Conv2D)      (None, 1, 1, 84)     193620      conv9_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc (Conv2D)  (None, 38, 38, 16)   73744       conv4_3_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_loc (Conv2D)           (None, 19, 19, 24)   221208      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc (Conv2D)       (None, 10, 10, 24)   110616      conv6_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc (Conv2D)       (None, 5, 5, 24)     55320       conv7_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc (Conv2D)       (None, 3, 3, 16)     36880       conv8_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc (Conv2D)       (None, 1, 1, 16)     36880       conv9_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf_reshape  (None, 5776, 21)     0           conv4_3_norm_mbox_conf[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_conf_reshape (Reshape) (None, 2166, 21)     0           fc7_mbox_conf[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf_reshape (Resh (None, 600, 21)      0           conv6_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf_reshape (Resh (None, 150, 21)      0           conv7_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf_reshape (Resh (None, 36, 21)       0           conv8_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf_reshape (Resh (None, 4, 21)        0           conv9_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox (Anc (None, 38, 38, 4, 8) 0           conv4_3_norm_mbox_loc[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox (AnchorBoxes) (None, 19, 19, 6, 8) 0           fc7_mbox_loc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox (AnchorBo (None, 10, 10, 6, 8) 0           conv6_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox (AnchorBo (None, 5, 5, 6, 8)   0           conv7_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox (AnchorBo (None, 3, 3, 4, 8)   0           conv8_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox (AnchorBo (None, 1, 1, 4, 8)   0           conv9_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mbox_conf (Concatenate)         (None, 8732, 21)     0           conv4_3_norm_mbox_conf_reshape[0]\n",
      "                                                                 fc7_mbox_conf_reshape[0][0]      \n",
      "                                                                 conv6_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv7_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv8_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv9_2_mbox_conf_reshape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc_reshape ( (None, 5776, 4)      0           conv4_3_norm_mbox_loc[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_loc_reshape (Reshape)  (None, 2166, 4)      0           fc7_mbox_loc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc_reshape (Resha (None, 600, 4)       0           conv6_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc_reshape (Resha (None, 150, 4)       0           conv7_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc_reshape (Resha (None, 36, 4)        0           conv8_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc_reshape (Resha (None, 4, 4)         0           conv9_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox_resh (None, 5776, 8)      0           conv4_3_norm_mbox_priorbox[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox_reshape (Resh (None, 2166, 8)      0           fc7_mbox_priorbox[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox_reshape ( (None, 600, 8)       0           conv6_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox_reshape ( (None, 150, 8)       0           conv7_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox_reshape ( (None, 36, 8)        0           conv8_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox_reshape ( (None, 4, 8)         0           conv9_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mbox_conf_softmax (Activation)  (None, 8732, 21)     0           mbox_conf[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mbox_loc (Concatenate)          (None, 8732, 4)      0           conv4_3_norm_mbox_loc_reshape[0][\n",
      "                                                                 fc7_mbox_loc_reshape[0][0]       \n",
      "                                                                 conv6_2_mbox_loc_reshape[0][0]   \n",
      "                                                                 conv7_2_mbox_loc_reshape[0][0]   \n",
      "                                                                 conv8_2_mbox_loc_reshape[0][0]   \n",
      "                                                                 conv9_2_mbox_loc_reshape[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "mbox_priorbox (Concatenate)     (None, 8732, 8)      0           conv4_3_norm_mbox_priorbox_reshap\n",
      "                                                                 fc7_mbox_priorbox_reshape[0][0]  \n",
      "                                                                 conv6_2_mbox_priorbox_reshape[0][\n",
      "                                                                 conv7_2_mbox_priorbox_reshape[0][\n",
      "                                                                 conv8_2_mbox_priorbox_reshape[0][\n",
      "                                                                 conv9_2_mbox_priorbox_reshape[0][\n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, 8732, 33)     0           mbox_conf_softmax[0][0]          \n",
      "                                                                 mbox_loc[0][0]                   \n",
      "                                                                 mbox_priorbox[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 26,285,486\n",
      "Trainable params: 26,285,486\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Set the path to the `.h5` file of the model to be loaded.\n",
    "# model_path = 'good_dropout_model/ssd300_dropout_pascal_07+12_epoch-114_loss-4.3685_val_loss-4.5034.h5'\n",
    "\n",
    "# # We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "# ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "# K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "# model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "#                                                'L2Normalization': L2Normalization,\n",
    "#                                                'DecodeDetections': DecodeDetections,\n",
    "#                                                'compute_loss': ssd_loss.compute_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load some images\n",
    "\n",
    "Load some images for which you'd like the model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image set 'val.txt': 100%|██████████| 5823/5823 [00:42<00:00, 135.54it/s]\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = '/usr/local/data/msmith/APL/Datasets/PASCAL/'\n",
    "# The directories that contain the images.\n",
    "VOC_2007_images_dir      = p.join(ROOT_PATH,'VOCdevkit/VOC2007/JPEGImages/')\n",
    "VOC_2012_images_dir      = p.join(ROOT_PATH,'VOCdevkit/VOC2012/JPEGImages/')\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "VOC_2007_annotations_dir      = p.join(ROOT_PATH,'VOCdevkit/VOC2007/Annotations/')\n",
    "VOC_2012_annotations_dir      = p.join(ROOT_PATH,'VOCdevkit/VOC2012/Annotations/')\n",
    "\n",
    "# The paths to the image sets.\n",
    "VOC_2007_train_image_set_filename    = p.join(ROOT_PATH,'VOCdevkit/VOC2007/ImageSets/Main/train.txt')\n",
    "VOC_2012_train_image_set_filename    = p.join(ROOT_PATH,'VOCdevkit/VOC2012/ImageSets/Main/train.txt')\n",
    "VOC_2007_val_image_set_filename      = p.join(ROOT_PATH,'VOCdevkit/VOC2007/ImageSets/Main/val.txt')\n",
    "VOC_2012_val_image_set_filename      = p.join(ROOT_PATH,'VOCdevkit/VOC2012/ImageSets/Main/val.txt')\n",
    "VOC_2007_trainval_image_set_filename = p.join(ROOT_PATH,'VOCdevkit/VOC2007/ImageSets/Main/trainval.txt')\n",
    "VOC_2012_trainval_image_set_filename = p.join(ROOT_PATH,'VOCdevkit/VOC2012/ImageSets/Main/trainval.txt')\n",
    "VOC_2007_test_image_set_filename     = p.join(ROOT_PATH,'VOCdevkit/VOC2007/ImageSets/Main/test.txt')\n",
    "\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "dataset = DataGenerator(load_images_into_memory=False)\n",
    "dataset.parse_xml(images_dirs=[VOC_2012_images_dir],\n",
    "                  image_set_filenames=[VOC_2012_val_image_set_filename],\n",
    "                  annotations_dirs=[VOC_2012_annotations_dir],\n",
    "                  classes=classes,\n",
    "                  include_classes='all',\n",
    "                  exclude_truncated=False,\n",
    "                  exclude_difficult=False,\n",
    "                  ret=False,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine resize because default version has weird behaviour\n",
    "class Resize:\n",
    "    '''\n",
    "    Resizes images to a specified height and width in pixels.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 height,\n",
    "                 width,\n",
    "                 interpolation_mode=cv2.INTER_LINEAR,\n",
    "                 box_filter=None,\n",
    "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
    "        '''\n",
    "        Arguments:\n",
    "            height (int): The desired height of the output images in pixels.\n",
    "            width (int): The desired width of the output images in pixels.\n",
    "            interpolation_mode (int, optional): An integer that denotes a valid\n",
    "                OpenCV interpolation mode. For example, integers 0 through 5 are\n",
    "                valid interpolation modes.\n",
    "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
    "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
    "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
    "                the validity of the bounding boxes is not checked.\n",
    "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
    "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
    "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
    "        '''\n",
    "        if not (isinstance(box_filter, BoxFilter) or box_filter is None):\n",
    "            raise ValueError(\"`box_filter` must be either `None` or a `BoxFilter` object.\")\n",
    "        self.out_height = height\n",
    "        self.out_width = width\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "        self.box_filter = box_filter\n",
    "        self.labels_format = labels_format\n",
    "\n",
    "    def __call__(self, image, labels=None, return_inverter=False):\n",
    "\n",
    "        img_height, img_width = image.shape[:2]\n",
    "\n",
    "        xmin = self.labels_format['xmin']\n",
    "        ymin = self.labels_format['ymin']\n",
    "        xmax = self.labels_format['xmax']\n",
    "        ymax = self.labels_format['ymax']\n",
    "\n",
    "        image = cv2.resize(image,\n",
    "                           dsize=(self.out_width, self.out_height),\n",
    "                           interpolation=self.interpolation_mode)\n",
    "\n",
    "        if return_inverter:\n",
    "            def inverter(labels):\n",
    "                labels = np.copy(labels)\n",
    "                labels[:, [ymin, ymax]] = np.round(labels[:, [ymin, ymax]] * (img_height / self.out_height), decimals=0)\n",
    "                labels[:, [xmin, xmax]] = np.round(labels[:, [xmin, xmax]] * (img_width / self.out_width), decimals=0)\n",
    "                return labels\n",
    "\n",
    "        if labels is None:\n",
    "            if return_inverter:\n",
    "                return image, inverter\n",
    "            else:\n",
    "                return image\n",
    "        else:\n",
    "            labels = np.copy(labels)\n",
    "            labels[:, [ymin, ymax]] = np.round(labels[:, [ymin, ymax]] * (self.out_height / img_height), decimals=0)\n",
    "            labels[:, [xmin, xmax]] = np.round(labels[:, [xmin, xmax]] * (self.out_width / img_width), decimals=0)\n",
    "\n",
    "            if not (self.box_filter is None):\n",
    "                self.box_filter.labels_format = self.labels_format\n",
    "                labels = self.box_filter(labels=labels,\n",
    "                                         image_height=self.out_height,\n",
    "                                         image_width=self.out_width)\n",
    "\n",
    "            if return_inverter:\n",
    "                return image, labels, inverter\n",
    "            else:\n",
    "                return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height,width=img_width, labels_format={'class_id': 0, 'xmin': -4, 'ymin': -3, 'xmax': -2, 'ymax': -1})\n",
    "\n",
    "transformations = [convert_to_3_channels, resize]\n",
    "\n",
    "batch_size = 128\n",
    "generator = dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         transformations=transformations,\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'original_images',\n",
    "                                                  'processed_images',\n",
    "                                                  'image_ids',\n",
    "                                                  'evaluation-neutral',\n",
    "                                                  'inverse_transform',\n",
    "                                                  'original_labels'},\n",
    "                                         keep_images_without_gt=True,\n",
    "                                         degenerate_box_handling='remove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = dataset.get_dataset_size()\n",
    "#n_images = 4*batch_size\n",
    "n_batches = int(np.ceil(n_images / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/usr/local/data/msmith/APL/dropout_object_detect/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Set the colors for the bounding boxes\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b915694c954a15b61ae08e76ee445a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=46), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d789bccbb515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Iterate over each batch image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mconcated\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Concated is now an array of shape (total num detections, num_class + 6)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Note that there will be many overlaps; we are now trying to remove them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(n_batches)):\n",
    "    batch_X, batch_image_ids, batch_eval_neutral, batch_inverse_transforms, original_X, batch_orig_labels = next(generator)\n",
    "    bs = len(batch_X)\n",
    "    \n",
    "    # After below for loop: nested list (Num passes, batch size)\n",
    "    # Each element : np array (num_detections, num_class+6)\n",
    "    model_out = []\n",
    "    for n in range(N):\n",
    "        model_out.append(decode_detections(model.predict(batch_X), confidence_thresh=0.1, \n",
    "                                      iou_threshold=0.1, top_k=200, normalize_coords=normalize_coords, \n",
    "                                      img_height=img_height, img_width=img_width))\n",
    "        \n",
    "    # Now list is an array (Num passes, batch size)\n",
    "    model_out = np.array(model_out)\n",
    "        \n",
    "    # Indexing : [image #][observation #]\n",
    "    # Note that each image will have different numbers of observations\n",
    "    # Each observation is an array with shape (num_detections, num_class+6)\n",
    "    observations_per_img = []\n",
    "    \n",
    "    # Iterate over each batch image\n",
    "    for i in range(bs):\n",
    "        concated =  np.concatenate([c for c in model_out[:,i] if 0 not in c.shape])\n",
    "        # Concated is now an array of shape (total num detections, num_class + 6)\n",
    "        # Note that there will be many overlaps; we are now trying to remove them\n",
    "        \n",
    "        observations = []\n",
    "        while concated.shape[0] > 0:\n",
    "            # Get first bounding box\n",
    "            box = concated[0, :]\n",
    "            # Calculate IoU for between said first box an all others\n",
    "            ious = iou(concated[:, -4:], box[-4:], coords='corners', border_pixels='half', mode='element-wise')\n",
    "            # Get matches, guaranteed one match min. to itself\n",
    "            keep_idx = ious >= 0.65\n",
    "            # Add box to list of boxes\n",
    "            # This way we are grouping all overlapping boxes\n",
    "            observations.append(concated[keep_idx, :])\n",
    "            # Remove the boxes we just found were overlapping (min. itself) and keep going\n",
    "            concated = concated[np.invert(keep_idx),:]\n",
    "        observations_per_img.append(observations)\n",
    "        \n",
    "    # Get label probabilities\n",
    "    observations_decoded_per_img = []\n",
    "    for i in range(bs):\n",
    "        observations_decoded = []\n",
    "        for obs in observations_per_img[i]:\n",
    "            # in observation: index 0-class id of max, 1-confidence of said class, 2-23 inclusive: softmax values, 24-27: box coordinates\n",
    "            means = np.mean(obs[:,2:], axis=0) # Get mean values of softmax and boxes per paper\n",
    "            ent = entropy(means[:-4]) # Get uncertainty estimate using entropy of mean (total uncertainty)\n",
    "            # Max entropy of 21 values slightly greater than 3\n",
    "            # Min is 0 obviously\n",
    "            new_class = np.argmax(means[1:-4]) + 1 # Get max class (ignoring background)\n",
    "            new_obs = np.empty((7))\n",
    "            new_obs[0:3] = [new_class, means[new_class] , ent]\n",
    "            new_obs[3:7] = means[-4:]\n",
    "            # New format: class id, associated class softmax confidence, entropy value, bounding box values\n",
    "            observations_decoded.append(new_obs)\n",
    "        observations_decoded = np.array(observations_decoded)\n",
    "        observations_decoded = observations_decoded[observations_decoded[:,1].argsort()]\n",
    "        observations_decoded_per_img.append(observations_decoded)\n",
    "        # Index format: [img #][observation #]\n",
    "\n",
    "    # Now transformed to original bounding box coord for display\n",
    "    observations_decoded_per_img = apply_inverse_transforms(observations_decoded_per_img, batch_inverse_transforms)\n",
    "        \n",
    "    \n",
    "    # Disable showing plot\n",
    "    plt.ioff()\n",
    "    #plt.ion()\n",
    "\n",
    "    # Display the image and draw the predicted boxes onto it.\n",
    "    for i in range(bs):\n",
    "        observations_decoded = observations_decoded_per_img[i]\n",
    "\n",
    "        plt.figure(figsize=(20,12))\n",
    "        plt.imshow(original_X[i])\n",
    "        plt.axis('off')\n",
    "\n",
    "        current_axis = plt.gca()\n",
    "\n",
    "        for observation in observations_decoded:\n",
    "            # Transform the predicted bounding boxes for the 300x300 image to the original image dimensions.\n",
    "            xmin = observation[-4]\n",
    "            ymin = observation[-3]\n",
    "            xmax = observation[-2]\n",
    "            ymax = observation[-1]\n",
    "            color = colors[int(observation[0])]\n",
    "            label = '{}: {:.2f} | {:.2f}'.format(classes[int(observation[0])], observation[1], observation[2])\n",
    "            current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "            current_axis.text(xmin, ymin, label, size='medium', color='white', bbox={'facecolor':color, 'alpha':1.0})\n",
    "\n",
    "        plt.savefig(os.path.join(save_dir, batch_image_ids[i] + '.pdf'), bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make predictions on Pascal VOC 2007 Test\n",
    "\n",
    "Let's use a `DataGenerator` to make predictions on the Pascal VOC 2007 test dataset and visualize the predicted boxes alongside the ground truth boxes for comparison. Everything here is preset already, but if you'd like to learn more about the data generator and its capabilities, take a look at the detailed tutorial in [this](https://github.com/pierluigiferrari/data_generator_object_detection_2d) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `BatchGenerator` instance and parse the Pascal VOC labels.\n",
    "\n",
    "dataset = DataGenerator()\n",
    "\n",
    "# TODO: Set the paths to the datasets here.\n",
    "\n",
    "VOC_2007_images_dir         = '../../datasets/VOCdevkit/VOC2007/JPEGImages/'\n",
    "VOC_2007_annotations_dir    = '../../datasets/VOCdevkit/VOC2007/Annotations/'\n",
    "VOC_2007_test_image_set_filename = '../../datasets/VOCdevkit/VOC2007/ImageSets/Main/test.txt'\n",
    "\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                  image_set_filenames=[VOC_2007_test_image_set_filename],\n",
    "                  annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                  classes=classes,\n",
    "                  include_classes='all',\n",
    "                  exclude_truncated=False,\n",
    "                  exclude_difficult=True,\n",
    "                  ret=False)\n",
    "\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "generator = dataset.generate(batch_size=1,\n",
    "                             shuffle=True,\n",
    "                             transformations=[convert_to_3_channels,\n",
    "                                              resize],\n",
    "                             returns={'processed_images',\n",
    "                                      'filenames',\n",
    "                                      'inverse_transform',\n",
    "                                      'original_images',\n",
    "                                      'original_labels'},\n",
    "                             keep_images_without_gt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch and make predictions.\n",
    "\n",
    "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(generator)\n",
    "\n",
    "i = 0 # Which batch item to look at\n",
    "\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(np.array(batch_original_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict.\n",
    "\n",
    "y_pred = model.predict(batch_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.5\n",
    "\n",
    "# Perform confidence thresholding.\n",
    "y_pred_thresh = [y_pred[k][y_pred[k,:,1] > confidence_threshold] for k in range(y_pred.shape[0])]\n",
    "\n",
    "# Convert the predictions for the original image.\n",
    "y_pred_thresh_inv = apply_inverse_transforms(y_pred_thresh, batch_inverse_transforms)\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('   class   conf xmin   ymin   xmax   ymax')\n",
    "print(y_pred_thresh_inv[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image and draw the predicted boxes onto it.\n",
    "\n",
    "# Set the colors for the bounding boxes\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.imshow(batch_original_images[i])\n",
    "\n",
    "current_axis = plt.gca()\n",
    "\n",
    "for box in batch_original_labels[i]:\n",
    "    xmin = box[1]\n",
    "    ymin = box[2]\n",
    "    xmax = box[3]\n",
    "    ymax = box[4]\n",
    "    label = '{}'.format(classes[int(box[0])])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
    "\n",
    "for box in y_pred_thresh_inv[i]:\n",
    "    xmin = box[2]\n",
    "    ymin = box[3]\n",
    "    xmax = box[4]\n",
    "    ymax = box[5]\n",
    "    color = colors[int(box[0])]\n",
    "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
