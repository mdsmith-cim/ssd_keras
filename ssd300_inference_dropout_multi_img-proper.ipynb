{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD300 Inference Tutorial\n",
    "\n",
    "This is a brief tutorial that shows how to use a trained SSD300 for inference on the Pascal VOC datasets. If you'd like more detailed explanations, please refer to [`ssd300_training.ipynb`](https://github.com/pierluigiferrari/ssd_keras/blob/master/ssd300_training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from scipy.stats import entropy\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import os.path as p\n",
    "import glob\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Model\n",
    "from ssd_encoder_decoder.ssd_output_decoder_dropout import decode_detections\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from data_generator.object_detection_2d_image_boxes_validation_utils import BoxFilter\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "\n",
    "from bounding_box_utils.bounding_box_utils import iou\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image size.\n",
    "img_height = 300\n",
    "img_width = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a trained SSD\n",
    "\n",
    "Either load a trained model or build a model and load trained weights into it. Since the HDF5 files I'm providing contain only the weights for the various SSD versions, not the complete models, you'll have to go with the latter option when using this implementation for the first time. You can then of course save the model and next time load the full model directly, without having to build it.\n",
    "\n",
    "You can find the download links to all the trained model weights in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Build the model and load trained weights into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True\n",
    "n_boxes = 8732\n",
    "N = 20 # Number of passes through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1123 11:14:24.124764 139717146937088 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W1123 11:14:24.125774 139717146937088 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1123 11:14:24.138833 139717146937088 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1123 11:14:24.140144 139717146937088 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1123 11:14:24.150514 139717146937088 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W1123 11:14:24.175775 139717146937088 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1123 11:14:24.328923 139717146937088 deprecation.py:506] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1123 11:14:26.796502 139717146937088 deprecation_wrapper.py:119] From /home/vision/msmith/localDrive/msmith/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1123 11:14:26.823361 139717146937088 deprecation.py:323] From /usr/local/data/msmith/uncertainty/ssd_keras/keras_loss_function/keras_ssd_loss.py:133: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1123 11:14:26.837772 139717146937088 deprecation.py:323] From /usr/local/data/msmith/uncertainty/ssd_keras/keras_loss_function/keras_ssd_loss.py:74: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1123 11:14:26.857185 139717146937088 deprecation.py:323] From /usr/local/data/msmith/uncertainty/ssd_keras/keras_loss_function/keras_ssd_loss.py:166: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                swap_channels=swap_channels)\n",
    "\n",
    "\n",
    "# 2: Load the trained weights into the model.\n",
    "\n",
    "# TODO: Set the path of the trained weights.\n",
    "weights_path = 'good_dropout_model/ssd300_dropout_PASCAL2012_train_+12_epoch-58_loss-3.8960_val_loss-5.0832.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Compile the model so that Keras won't complain the next time you load it.\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 300, 300, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "identity_layer (Lambda)         (None, 300, 300, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_mean_normalization (Lambd (None, 300, 300, 3)  0           identity_layer[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_channel_swap (Lambda)     (None, 300, 300, 3)  0           input_mean_normalization[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                (None, 300, 300, 64) 1792        input_channel_swap[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2 (Conv2D)                (None, 300, 300, 64) 36928       conv1_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 150, 150, 64) 0           conv1_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1 (Conv2D)                (None, 150, 150, 128 73856       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2 (Conv2D)                (None, 150, 150, 128 147584      conv2_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, 75, 75, 128)  0           conv2_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1 (Conv2D)                (None, 75, 75, 256)  295168      pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2 (Conv2D)                (None, 75, 75, 256)  590080      conv3_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3 (Conv2D)                (None, 75, 75, 256)  590080      conv3_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, 38, 38, 256)  0           conv3_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1 (Conv2D)                (None, 38, 38, 512)  1180160     pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3 (Conv2D)                (None, 38, 38, 512)  2359808     conv4_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling2D)            (None, 19, 19, 512)  0           conv4_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1 (Conv2D)                (None, 19, 19, 512)  2359808     pool4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3 (Conv2D)                (None, 19, 19, 512)  2359808     conv5_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pool5 (MaxPooling2D)            (None, 19, 19, 512)  0           conv5_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fc6 (Conv2D)                    (None, 19, 19, 1024) 4719616     pool5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 19, 19, 1024) 0           fc6[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "fc7 (Conv2D)                    (None, 19, 19, 1024) 1049600     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 19, 19, 1024) 0           fc7[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv6_1 (Conv2D)                (None, 19, 19, 256)  262400      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv6_padding (ZeroPadding2D)   (None, 21, 21, 256)  0           conv6_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2 (Conv2D)                (None, 10, 10, 512)  1180160     conv6_padding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv7_1 (Conv2D)                (None, 10, 10, 128)  65664       conv6_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_padding (ZeroPadding2D)   (None, 12, 12, 128)  0           conv7_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2 (Conv2D)                (None, 5, 5, 256)    295168      conv7_padding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv8_1 (Conv2D)                (None, 5, 5, 128)    32896       conv7_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2 (Conv2D)                (None, 3, 3, 256)    295168      conv8_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_1 (Conv2D)                (None, 3, 3, 128)    32896       conv8_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm (L2Normalization)  (None, 38, 38, 512)  512         conv4_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2 (Conv2D)                (None, 1, 1, 256)    295168      conv9_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf (Conv2D) (None, 38, 38, 84)   387156      conv4_3_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_conf (Conv2D)          (None, 19, 19, 126)  1161342     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf (Conv2D)      (None, 10, 10, 126)  580734      conv6_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf (Conv2D)      (None, 5, 5, 126)    290430      conv7_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf (Conv2D)      (None, 3, 3, 84)     193620      conv8_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf (Conv2D)      (None, 1, 1, 84)     193620      conv9_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc (Conv2D)  (None, 38, 38, 16)   73744       conv4_3_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_loc (Conv2D)           (None, 19, 19, 24)   221208      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc (Conv2D)       (None, 10, 10, 24)   110616      conv6_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc (Conv2D)       (None, 5, 5, 24)     55320       conv7_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc (Conv2D)       (None, 3, 3, 16)     36880       conv8_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc (Conv2D)       (None, 1, 1, 16)     36880       conv9_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_conf_reshape  (None, 5776, 21)     0           conv4_3_norm_mbox_conf[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_conf_reshape (Reshape) (None, 2166, 21)     0           fc7_mbox_conf[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_conf_reshape (Resh (None, 600, 21)      0           conv6_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_conf_reshape (Resh (None, 150, 21)      0           conv7_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_conf_reshape (Resh (None, 36, 21)       0           conv8_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_conf_reshape (Resh (None, 4, 21)        0           conv9_2_mbox_conf[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox (Anc (None, 38, 38, 4, 8) 0           conv4_3_norm_mbox_loc[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox (AnchorBoxes) (None, 19, 19, 6, 8) 0           fc7_mbox_loc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox (AnchorBo (None, 10, 10, 6, 8) 0           conv6_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox (AnchorBo (None, 5, 5, 6, 8)   0           conv7_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox (AnchorBo (None, 3, 3, 4, 8)   0           conv8_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox (AnchorBo (None, 1, 1, 4, 8)   0           conv9_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mbox_conf (Concatenate)         (None, 8732, 21)     0           conv4_3_norm_mbox_conf_reshape[0]\n",
      "                                                                 fc7_mbox_conf_reshape[0][0]      \n",
      "                                                                 conv6_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv7_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv8_2_mbox_conf_reshape[0][0]  \n",
      "                                                                 conv9_2_mbox_conf_reshape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_loc_reshape ( (None, 5776, 4)      0           conv4_3_norm_mbox_loc[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_loc_reshape (Reshape)  (None, 2166, 4)      0           fc7_mbox_loc[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_loc_reshape (Resha (None, 600, 4)       0           conv6_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_loc_reshape (Resha (None, 150, 4)       0           conv7_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_loc_reshape (Resha (None, 36, 4)        0           conv8_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_loc_reshape (Resha (None, 4, 4)         0           conv9_2_mbox_loc[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_norm_mbox_priorbox_resh (None, 5776, 8)      0           conv4_3_norm_mbox_priorbox[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "fc7_mbox_priorbox_reshape (Resh (None, 2166, 8)      0           fc7_mbox_priorbox[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv6_2_mbox_priorbox_reshape ( (None, 600, 8)       0           conv6_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv7_2_mbox_priorbox_reshape ( (None, 150, 8)       0           conv7_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv8_2_mbox_priorbox_reshape ( (None, 36, 8)        0           conv8_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv9_2_mbox_priorbox_reshape ( (None, 4, 8)         0           conv9_2_mbox_priorbox[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mbox_conf_softmax (Activation)  (None, 8732, 21)     0           mbox_conf[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "mbox_loc (Concatenate)          (None, 8732, 4)      0           conv4_3_norm_mbox_loc_reshape[0][\n",
      "                                                                 fc7_mbox_loc_reshape[0][0]       \n",
      "                                                                 conv6_2_mbox_loc_reshape[0][0]   \n",
      "                                                                 conv7_2_mbox_loc_reshape[0][0]   \n",
      "                                                                 conv8_2_mbox_loc_reshape[0][0]   \n",
      "                                                                 conv9_2_mbox_loc_reshape[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "mbox_priorbox (Concatenate)     (None, 8732, 8)      0           conv4_3_norm_mbox_priorbox_reshap\n",
      "                                                                 fc7_mbox_priorbox_reshape[0][0]  \n",
      "                                                                 conv6_2_mbox_priorbox_reshape[0][\n",
      "                                                                 conv7_2_mbox_priorbox_reshape[0][\n",
      "                                                                 conv8_2_mbox_priorbox_reshape[0][\n",
      "                                                                 conv9_2_mbox_priorbox_reshape[0][\n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, 8732, 33)     0           mbox_conf_softmax[0][0]          \n",
      "                                                                 mbox_loc[0][0]                   \n",
      "                                                                 mbox_priorbox[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 26,285,486\n",
      "Trainable params: 26,285,486\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Set the path to the `.h5` file of the model to be loaded.\n",
    "# model_path = 'good_dropout_model/ssd300_dropout_pascal_07+12_epoch-114_loss-4.3685_val_loss-4.5034.h5'\n",
    "\n",
    "# # We need to create an SSDLoss object in order to pass that to the model loader.\n",
    "# ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "# K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "# model = load_model(model_path, custom_objects={'AnchorBoxes': AnchorBoxes,\n",
    "#                                                'L2Normalization': L2Normalization,\n",
    "#                                                'DecodeDetections': DecodeDetections,\n",
    "#                                                'compute_loss': ssd_loss.compute_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load some images\n",
    "\n",
    "Load some images for which you'd like the model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image set 'val.txt': 100%|██████████| 5823/5823 [00:46<00:00, 125.75it/s]\n",
      "Loading images into memory: 100%|██████████| 5823/5823 [01:20<00:00, 72.26it/s]\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = '/usr/local/data/msmith/APL/Datasets/PASCAL/'\n",
    "# The directories that contain the images.\n",
    "VOC_2007_images_dir      = p.join(ROOT_PATH,'VOCdevkit/VOC2007/JPEGImages/')\n",
    "VOC_2012_images_dir      = p.join(ROOT_PATH,'VOCdevkit/VOC2012/JPEGImages/')\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "VOC_2007_annotations_dir      = p.join(ROOT_PATH,'VOCdevkit/VOC2007/Annotations/')\n",
    "VOC_2012_annotations_dir      = p.join(ROOT_PATH,'VOCdevkit/VOC2012/Annotations/')\n",
    "\n",
    "# The paths to the image sets.\n",
    "VOC_2007_train_image_set_filename    = p.join(ROOT_PATH,'VOCdevkit/VOC2007/ImageSets/Main/train.txt')\n",
    "VOC_2012_train_image_set_filename    = p.join(ROOT_PATH,'VOCdevkit/VOC2012/ImageSets/Main/train.txt')\n",
    "VOC_2007_val_image_set_filename      = p.join(ROOT_PATH,'VOCdevkit/VOC2007/ImageSets/Main/val.txt')\n",
    "VOC_2012_val_image_set_filename      = p.join(ROOT_PATH,'VOCdevkit/VOC2012/ImageSets/Main/val.txt')\n",
    "VOC_2007_trainval_image_set_filename = p.join(ROOT_PATH,'VOCdevkit/VOC2007/ImageSets/Main/trainval.txt')\n",
    "VOC_2012_trainval_image_set_filename = p.join(ROOT_PATH,'VOCdevkit/VOC2012/ImageSets/Main/trainval.txt')\n",
    "VOC_2007_test_image_set_filename     = p.join(ROOT_PATH,'VOCdevkit/VOC2007/ImageSets/Main/test.txt')\n",
    "\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "dataset = DataGenerator(load_images_into_memory=True)\n",
    "dataset.parse_xml(images_dirs=[VOC_2012_images_dir],\n",
    "                  image_set_filenames=[VOC_2012_val_image_set_filename],\n",
    "                  annotations_dirs=[VOC_2012_annotations_dir],\n",
    "                  classes=classes,\n",
    "                  include_classes='all',\n",
    "                  exclude_truncated=False,\n",
    "                  exclude_difficult=False,\n",
    "                  ret=False,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine resize because default version has weird behaviour\n",
    "class Resize:\n",
    "    '''\n",
    "    Resizes images to a specified height and width in pixels.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 height,\n",
    "                 width,\n",
    "                 interpolation_mode=cv2.INTER_LINEAR,\n",
    "                 box_filter=None,\n",
    "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
    "        '''\n",
    "        Arguments:\n",
    "            height (int): The desired height of the output images in pixels.\n",
    "            width (int): The desired width of the output images in pixels.\n",
    "            interpolation_mode (int, optional): An integer that denotes a valid\n",
    "                OpenCV interpolation mode. For example, integers 0 through 5 are\n",
    "                valid interpolation modes.\n",
    "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
    "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
    "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
    "                the validity of the bounding boxes is not checked.\n",
    "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
    "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
    "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
    "        '''\n",
    "        if not (isinstance(box_filter, BoxFilter) or box_filter is None):\n",
    "            raise ValueError(\"`box_filter` must be either `None` or a `BoxFilter` object.\")\n",
    "        self.out_height = height\n",
    "        self.out_width = width\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "        self.box_filter = box_filter\n",
    "        self.labels_format = labels_format\n",
    "\n",
    "    def __call__(self, image, labels=None, return_inverter=False):\n",
    "\n",
    "        img_height, img_width = image.shape[:2]\n",
    "\n",
    "        xmin = self.labels_format['xmin']\n",
    "        ymin = self.labels_format['ymin']\n",
    "        xmax = self.labels_format['xmax']\n",
    "        ymax = self.labels_format['ymax']\n",
    "\n",
    "        image = cv2.resize(image,\n",
    "                           dsize=(self.out_width, self.out_height),\n",
    "                           interpolation=self.interpolation_mode)\n",
    "\n",
    "        if return_inverter:\n",
    "            def inverter(labels):\n",
    "                labels = np.copy(labels)\n",
    "                labels[:, [ymin, ymax]] = np.round(labels[:, [ymin, ymax]] * (img_height / self.out_height), decimals=0)\n",
    "                labels[:, [xmin, xmax]] = np.round(labels[:, [xmin, xmax]] * (img_width / self.out_width), decimals=0)\n",
    "                return labels\n",
    "\n",
    "        if labels is None:\n",
    "            if return_inverter:\n",
    "                return image, inverter\n",
    "            else:\n",
    "                return image\n",
    "        else:\n",
    "            labels = np.copy(labels)\n",
    "            labels[:, [ymin, ymax]] = np.round(labels[:, [ymin, ymax]] * (self.out_height / img_height), decimals=0)\n",
    "            labels[:, [xmin, xmax]] = np.round(labels[:, [xmin, xmax]] * (self.out_width / img_width), decimals=0)\n",
    "\n",
    "            if not (self.box_filter is None):\n",
    "                self.box_filter.labels_format = self.labels_format\n",
    "                labels = self.box_filter(labels=labels,\n",
    "                                         image_height=self.out_height,\n",
    "                                         image_width=self.out_width)\n",
    "\n",
    "            if return_inverter:\n",
    "                return image, labels, inverter\n",
    "            else:\n",
    "                return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height,width=img_width, labels_format={'class_id': 0, 'xmin': -4, 'ymin': -3, 'xmax': -2, 'ymax': -1})\n",
    "\n",
    "transformations = [convert_to_3_channels, resize]\n",
    "\n",
    "batch_size = 128\n",
    "generator = dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         transformations=transformations,\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'original_images',\n",
    "                                                  'processed_images',\n",
    "                                                  'image_ids',\n",
    "                                                  'evaluation-neutral',\n",
    "                                                  'inverse_transform',\n",
    "                                                  'original_labels'},\n",
    "                                         keep_images_without_gt=True,\n",
    "                                         degenerate_box_handling='remove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = dataset.get_dataset_size()\n",
    "#n_images = 4*batch_size\n",
    "n_batches = int(np.ceil(n_images / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/usr/local/data/msmith/APL/dropout_object_detect/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Set the colors for the bounding boxes\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runInference(saveFigures=True):\n",
    "    results = []\n",
    "    for _ in tqdm(range(n_batches)):\n",
    "        batch_X, batch_image_ids, batch_eval_neutral, batch_inverse_transforms, original_X, batch_orig_labels = next(generator)\n",
    "        bs = len(batch_X)\n",
    "\n",
    "        # After below for loop: nested list (Num passes, batch size)\n",
    "        # Each element : np array (num_detections, num_class+6)\n",
    "        model_out = []\n",
    "        for n in range(N):\n",
    "            model_out.append(decode_detections(model.predict(batch_X), confidence_thresh=0.1, \n",
    "                                          iou_threshold=0.1, top_k=200, normalize_coords=normalize_coords, \n",
    "                                          img_height=img_height, img_width=img_width))\n",
    "\n",
    "        # Now list is an array (Num passes, batch size)\n",
    "        model_out = np.array(model_out)\n",
    "\n",
    "        # Indexing : [image #][observation #]\n",
    "        # Note that each image will have different numbers of observations\n",
    "        # Each observation is an array with shape (num_detections, num_class+6)\n",
    "        observations_per_img = []\n",
    "\n",
    "        # Iterate over each batch image\n",
    "        for i in range(bs):\n",
    "            filtered = [c for c in model_out[:,i] if 0 not in c.shape]\n",
    "            if len(filtered) == 0:\n",
    "                concated = np.empty((0))\n",
    "            else:\n",
    "                concated =  np.concatenate([c for c in model_out[:,i] if 0 not in c.shape])\n",
    "            # Concated is now an array of shape (total num detections, num_class + 6)\n",
    "            # Note that there will be many overlaps; we are now trying to remove them\n",
    "\n",
    "            observations = []\n",
    "            while concated.shape[0] > 0:\n",
    "                # Get first bounding box\n",
    "                box = concated[0, :]\n",
    "                # Calculate IoU for between said first box an all others\n",
    "                ious = iou(concated[:, -4:], box[-4:], coords='corners', border_pixels='half', mode='element-wise')\n",
    "                # Get matches, guaranteed one match min. to itself\n",
    "                keep_idx = ious >= 0.65\n",
    "                # Add box to list of boxes\n",
    "                # This way we are grouping all overlapping boxes\n",
    "                observations.append(concated[keep_idx, :])\n",
    "                # Remove the boxes we just found were overlapping (min. itself) and keep going\n",
    "                concated = concated[np.invert(keep_idx),:]\n",
    "            observations_per_img.append(observations)\n",
    "\n",
    "        # Get label probabilities\n",
    "        observations_decoded_per_img = []\n",
    "        for i in range(bs):\n",
    "            observations_decoded = []\n",
    "            for obs in observations_per_img[i]:\n",
    "                # in observation: index 0-class id of max, 1-confidence of said class, 2-23 inclusive: softmax values, 24-27: box coordinates\n",
    "                means = np.mean(obs[:,2:], axis=0) # Get mean values of softmax and boxes per paper\n",
    "                ent = entropy(means[:-4]) # Get uncertainty estimate using entropy of mean (total uncertainty)\n",
    "                # Max entropy of 21 values slightly greater than 3\n",
    "                # Min is 0 obviously\n",
    "                new_class = np.argmax(means[1:-4]) + 1 # Get max class (ignoring background)\n",
    "                new_obs = np.empty((7))\n",
    "                new_obs[0:3] = [new_class, means[new_class] , ent]\n",
    "                new_obs[3:7] = means[-4:]\n",
    "                # New format: class id, associated class softmax confidence, entropy value, bounding box values\n",
    "                observations_decoded.append(new_obs)\n",
    "            observations_decoded = np.array(observations_decoded)\n",
    "            # If not empty\n",
    "            if len(observations_decoded) > 0:\n",
    "                observations_decoded = observations_decoded[observations_decoded[:,1].argsort()]\n",
    "            observations_decoded_per_img.append(observations_decoded)\n",
    "            # Index format: [img #][observation #]\n",
    "\n",
    "        # Now transformed to original bounding box coord for display\n",
    "        # Index: [img #][detection #](class id, associated class softmax confidence, entropy value, bounding box values)\n",
    "        observations_decoded_per_img = apply_inverse_transforms(observations_decoded_per_img, batch_inverse_transforms)\n",
    "        \n",
    "        # Save data for later processing\n",
    "        results.append((observations_decoded_per_img, batch_image_ids, batch_orig_labels))\n",
    "        \n",
    "        if saveFigures:\n",
    "            # Disable showing plot\n",
    "            plt.ioff()\n",
    "            #plt.ion()\n",
    "\n",
    "            # Display the image and draw the predicted boxes onto it.\n",
    "            for i in range(bs):\n",
    "                observations_decoded = observations_decoded_per_img[i]\n",
    "\n",
    "                save_file = Path(os.path.join(save_dir, batch_image_ids[i] + '.pdf'))\n",
    "                save_file_jpg = Path(os.path.join(save_dir, batch_image_ids[i] + '.jpg'))\n",
    "                if not save_file.exists():\n",
    "                    plt.figure(figsize=(20,12))\n",
    "                    plt.imshow(original_X[i])\n",
    "                    plt.axis('off')\n",
    "\n",
    "                    current_axis = plt.gca()\n",
    "\n",
    "                    for observation in observations_decoded:\n",
    "                        # Transform the predicted bounding boxes for the 300x300 image to the original image dimensions.\n",
    "                        xmin = observation[-4]\n",
    "                        ymin = observation[-3]\n",
    "                        xmax = observation[-2]\n",
    "                        ymax = observation[-1]\n",
    "                        color = colors[int(observation[0])]\n",
    "                        label = '{}: {:.2f} | {:.2f}'.format(classes[int(observation[0])], observation[1], observation[2])\n",
    "                        current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "                        current_axis.text(xmin, ymin, label, size='medium', color='white', bbox={'facecolor':color, 'alpha':1.0})\n",
    "\n",
    "                    plt.savefig(save_file, bbox_inches='tight', pad_inches=0)\n",
    "                    plt.savefig(save_file_jpg, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "                    plt.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f285f157e64ecd8104a934b5ed0ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=46), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = runInference(False)\n",
    "\n",
    "# Convert output to more usable format indexed by image number\n",
    "prediction_list = []\n",
    "id_list = []\n",
    "gt_list = []\n",
    "for i in results:\n",
    "    prediction_list.extend(i[0])\n",
    "    id_list.extend(i[1])\n",
    "    gt_list.extend(i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gt_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}\n",
    "def get_num_gt_per_class(verbose=True):\n",
    "    '''\n",
    "    Counts the number of ground truth boxes for each class across the dataset.\n",
    "\n",
    "    Arguments:\n",
    "        verbose (bool, optional): If `True`, will print out the progress during runtime.\n",
    "\n",
    "    Returns:\n",
    "        A list containing a count of the number of ground truth boxes for each class across the\n",
    "        entire dataset.\n",
    "    '''\n",
    "\n",
    "    num_gt_per_class = np.zeros(shape=(n_classes+1), dtype=np.int)\n",
    "\n",
    "    # Iterate over the ground truth for all images in the dataset.\n",
    "    for i in tqdm(range(len(gt_list))):\n",
    "\n",
    "        boxes = np.asarray(gt_list[i])\n",
    "\n",
    "        # Iterate over all ground truth boxes for the current image.\n",
    "        for j in range(boxes.shape[0]):\n",
    "            class_id = boxes[j, 0]\n",
    "            num_gt_per_class[class_id] += 1\n",
    "\n",
    "    return num_gt_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880a2c3f4ab64a549d91c560d8d96e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5823), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_gt_per_class = get_num_gt_per_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_predictions(prediction_list, id_list, gt_list, \n",
    "                      matching_iou_threshold=0.5,\n",
    "                      border_pixels='include',\n",
    "                      sorting_algorithm='quicksort'):\n",
    "    '''\n",
    "    Matches predictions to ground truth boxes.\n",
    "\n",
    "    Note that `predict_on_dataset()` must be called before calling this method.\n",
    "\n",
    "    Arguments:\n",
    "        matching_iou_threshold (float, optional): A prediction will be considered a true positive if it has a Jaccard overlap\n",
    "            of at least `matching_iou_threshold` with any ground truth bounding box of the same class.\n",
    "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
    "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
    "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
    "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
    "            to the boxex, but not the other.\n",
    "        sorting_algorithm (str, optional): Which sorting algorithm the matching algorithm should use. This argument accepts\n",
    "            any valid sorting algorithm for Numpy's `argsort()` function. You will usually want to choose between 'quicksort'\n",
    "            (fastest and most memory efficient, but not stable) and 'mergesort' (slight slower and less memory efficient, but stable).\n",
    "            The official Matlab evaluation algorithm uses a stable sorting algorithm, so this algorithm is only guaranteed\n",
    "            to behave identically if you choose 'mergesort' as the sorting algorithm, but it will almost always behave identically\n",
    "            even if you choose 'quicksort' (but no guarantees).\n",
    "\n",
    "    Returns:\n",
    "        Four nested lists containing the true positives, false positives, cumulative true positives,\n",
    "        and cumulative false positives for each class.\n",
    "    '''\n",
    "\n",
    "    # Convert the ground truth to a more efficient format for what we need\n",
    "    # to do, which is access ground truth by image ID repeatedly.\n",
    "    ground_truth = {}\n",
    "    for i in range(n_images):\n",
    "        image_id = str(id_list[i])\n",
    "        labels = gt_list[i]\n",
    "        ground_truth[image_id] = np.asarray(labels)\n",
    "\n",
    "    true_positives = [[]] # The false positives for each class, sorted by descending confidence.\n",
    "    false_positives = [[]] # The true positives for each class, sorted by descending confidence.\n",
    "    cumulative_true_positives = [[]]\n",
    "    cumulative_false_positives = [[]]\n",
    "\n",
    "    # Iterate over all images\n",
    "    predictions_by_class = [list() for _ in range(n_classes + 1)]\n",
    "    for k, prediction in enumerate(prediction_list):\n",
    "\n",
    "        image_id = id_list[k]\n",
    "\n",
    "        for box in prediction:\n",
    "            class_id = int(box[0])\n",
    "            confidence = box[1]\n",
    "            ent = box[2]\n",
    "            xmin = round(box[-4], 1)\n",
    "            ymin = round(box[-3], 1)\n",
    "            xmax = round(box[-2], 1)\n",
    "            ymax = round(box[-1], 1)\n",
    "            prediction = (image_id, confidence, ent, xmin, ymin, xmax, ymax)\n",
    "            # Append the predicted box to the results list for its class.\n",
    "            predictions_by_class[class_id].append(prediction)\n",
    "    \n",
    "    # Iterate over all classes.\n",
    "    for class_id in tqdm(range(1, n_classes + 1)):\n",
    "\n",
    "        predict_by_class = predictions_by_class[class_id]\n",
    "\n",
    "        # Store the matching results in these lists:\n",
    "        true_pos = np.zeros(len(predict_by_class), dtype=np.int) # 1 for every prediction that is a true positive, 0 otherwise\n",
    "        false_pos = np.zeros(len(predict_by_class), dtype=np.int) # 1 for every prediction that is a false positive, 0 otherwise\n",
    "\n",
    "        # In case there are no predictions at all for this class, we're done here.\n",
    "        if len(predict_by_class) == 0:\n",
    "            print(\"No predictions for class {}/{}\".format(class_id, n_classes))\n",
    "            true_positives.append(true_pos)\n",
    "            false_positives.append(false_pos)\n",
    "            continue\n",
    "\n",
    "        # Convert the predictions list for this class into a structured array so that we can sort it by confidence.\n",
    "\n",
    "        # Get the number of characters needed to store the image ID strings in the structured array.\n",
    "        num_chars_per_image_id = len(str(predict_by_class[0][0])) + 6 # Keep a few characters buffer in case some image IDs are longer than others.\n",
    "        # Create the data type for the structured array.\n",
    "        preds_data_type = np.dtype([('image_id', 'U{}'.format(num_chars_per_image_id)),\n",
    "                                    ('confidence', 'f4'),\n",
    "                                    ('entropy', 'f4'),\n",
    "                                    ('xmin', 'f4'),\n",
    "                                    ('ymin', 'f4'),\n",
    "                                    ('xmax', 'f4'),\n",
    "                                    ('ymax', 'f4')])\n",
    "        # Create the structured array\n",
    "        predict_by_class = np.array(predict_by_class, dtype=preds_data_type)\n",
    "\n",
    "        # Sort the detections by decreasing confidence.\n",
    "        descending_indices = np.argsort(-predict_by_class['confidence'], kind=sorting_algorithm)\n",
    "        predictions_sorted = predict_by_class[descending_indices]\n",
    "\n",
    "        # Keep track of which ground truth boxes were already matched to a detection.\n",
    "        gt_matched = {}\n",
    "\n",
    "        # Iterate over all predictions.\n",
    "        for i in range(len(predict_by_class)):\n",
    "\n",
    "            prediction = predictions_sorted[i]\n",
    "            image_id = prediction['image_id']\n",
    "            pred_box = np.asarray(list(prediction[['xmin', 'ymin', 'xmax', 'ymax']])) # Convert the structured array element to a regular array.\n",
    "\n",
    "            # Get the relevant ground truth boxes for this prediction,\n",
    "            # i.e. all ground truth boxes that match the prediction's\n",
    "            # image ID and class ID.\n",
    "\n",
    "            gt = ground_truth[image_id]\n",
    "            gt = np.asarray(gt)\n",
    "            class_mask = gt[:,0] == class_id\n",
    "            gt = gt[class_mask]\n",
    "\n",
    "            if gt.size == 0:\n",
    "                # If the image doesn't contain any objects of this class,\n",
    "                # the prediction becomes a false positive.\n",
    "                false_pos[i] = 1\n",
    "                continue\n",
    "\n",
    "            # Compute the IoU of this prediction with all ground truth boxes of the same class.\n",
    "            overlaps = iou(boxes1=gt[:,[1, 2, 3, 4]],\n",
    "                           boxes2=pred_box,\n",
    "                           coords='corners',\n",
    "                           mode='element-wise',\n",
    "                           border_pixels=border_pixels)\n",
    "\n",
    "            # For each detection, match the ground truth box with the highest overlap.\n",
    "            # It's possible that the same ground truth box will be matched to multiple\n",
    "            # detections.\n",
    "            gt_match_index = np.argmax(overlaps)\n",
    "            gt_match_overlap = overlaps[gt_match_index]\n",
    "\n",
    "            if gt_match_overlap < matching_iou_threshold:\n",
    "                # False positive, IoU threshold violated:\n",
    "                # Those predictions whose matched overlap is below the threshold become\n",
    "                # false positives.\n",
    "                false_pos[i] = 1\n",
    "            else:\n",
    "                if not (image_id in gt_matched):\n",
    "                    # True positive:\n",
    "                    # If the matched ground truth box for this prediction hasn't been matched to a\n",
    "                    # different prediction already, we have a true positive.\n",
    "                    true_pos[i] = 1\n",
    "                    gt_matched[image_id] = np.zeros(shape=(gt.shape[0]), dtype=np.bool)\n",
    "                    gt_matched[image_id][gt_match_index] = True\n",
    "                elif not gt_matched[image_id][gt_match_index]:\n",
    "                    # True positive:\n",
    "                    # If the matched ground truth box for this prediction hasn't been matched to a\n",
    "                    # different prediction already, we have a true positive.\n",
    "                    true_pos[i] = 1\n",
    "                    gt_matched[image_id][gt_match_index] = True\n",
    "                else:\n",
    "                    # False positive, duplicate detection:\n",
    "                    # If the matched ground truth box for this prediction has already been matched\n",
    "                    # to a different prediction previously, it is a duplicate detection for an\n",
    "                    # already detected object, which counts as a false positive.\n",
    "                    false_pos[i] = 1\n",
    "\n",
    "        true_positives.append(true_pos)\n",
    "        false_positives.append(false_pos)\n",
    "\n",
    "        cumulative_true_pos = np.cumsum(true_pos) # Cumulative sums of the true positives\n",
    "        cumulative_false_pos = np.cumsum(false_pos) # Cumulative sums of the false positives\n",
    "\n",
    "        cumulative_true_positives.append(cumulative_true_pos)\n",
    "        cumulative_false_positives.append(cumulative_false_pos)\n",
    "\n",
    "    return true_positives, false_positives, cumulative_true_positives, cumulative_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18f5426914548c19c427f65f8ca04a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tp, fp, cum_tp, cum_fp = match_predictions(prediction_list, id_list, gt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(814,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(814,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 134, 135, 136, 137, 138, 139, 140, 141,\n",
       "       141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "       154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166,\n",
       "       167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179,\n",
       "       180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192,\n",
       "       193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
       "       206, 207, 208, 209, 210, 211, 212, 213, 214, 214, 215, 216, 217,\n",
       "       218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230,\n",
       "       231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 241, 242,\n",
       "       243, 244, 244, 245, 246, 246, 247, 248, 249, 250, 251, 252, 253,\n",
       "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 264, 264,\n",
       "       265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277,\n",
       "       277, 277, 278, 279, 279, 280, 280, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 292, 293, 294, 295, 295, 296,\n",
       "       297, 298, 299, 300, 301, 301, 301, 302, 302, 302, 303, 304, 305,\n",
       "       306, 307, 308, 308, 308, 308, 309, 310, 310, 310, 311, 312, 313,\n",
       "       314, 315, 316, 317, 317, 318, 319, 320, 321, 322, 323, 323, 323,\n",
       "       324, 325, 326, 327, 327, 327, 327, 327, 328, 329, 329, 329, 329,\n",
       "       329, 330, 331, 332, 333, 334, 334, 334, 334, 334, 334, 335, 336,\n",
       "       337, 337, 338, 338, 338, 339, 340, 340, 341, 342, 343, 344, 344,\n",
       "       345, 345, 346, 346, 346, 347, 347, 347, 348, 348, 348, 348, 348,\n",
       "       348, 348, 349, 349, 349, 349, 349, 349, 349, 349, 349, 349, 349,\n",
       "       349, 349, 349, 350, 350, 351, 351, 351, 351, 351, 351, 352, 352,\n",
       "       352, 353, 353, 353, 353, 353, 353, 353, 353, 353, 353, 354, 354,\n",
       "       354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 354, 355,\n",
       "       355, 355, 355, 355, 355, 355, 355, 355, 355, 355, 355, 355, 355,\n",
       "       355, 355, 355, 355, 355, 355, 355, 355, 355, 355, 355, 356, 356,\n",
       "       356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356, 356,\n",
       "       356, 356, 356, 356, 356, 356])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_tp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   2,   2,   2,   2,   2,   2,   2,   2,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
       "         3,   3,   3,   3,   3,   3,   3,   3,   3,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "         4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   5,   5,\n",
       "         5,   5,   6,   6,   6,   7,   7,   7,   7,   7,   7,   7,   7,\n",
       "         7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   7,   8,   9,\n",
       "         9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,   9,\n",
       "        10,  11,  11,  11,  12,  12,  13,  14,  14,  14,  14,  14,  14,\n",
       "        14,  14,  14,  14,  14,  14,  14,  15,  15,  15,  15,  16,  16,\n",
       "        16,  16,  16,  16,  16,  17,  18,  18,  19,  20,  20,  20,  20,\n",
       "        20,  20,  20,  21,  22,  23,  23,  23,  24,  25,  25,  25,  25,\n",
       "        25,  25,  25,  25,  26,  26,  26,  26,  26,  26,  26,  27,  28,\n",
       "        28,  28,  28,  28,  29,  30,  31,  32,  32,  32,  33,  34,  35,\n",
       "        36,  36,  36,  36,  36,  36,  37,  38,  39,  40,  41,  41,  41,\n",
       "        41,  42,  42,  43,  44,  44,  44,  45,  45,  45,  45,  45,  46,\n",
       "        46,  47,  47,  48,  49,  49,  50,  51,  51,  52,  53,  54,  55,\n",
       "        56,  57,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,\n",
       "        68,  69,  70,  70,  71,  71,  72,  73,  74,  75,  76,  76,  77,\n",
       "        78,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  87,  88,\n",
       "        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 100,\n",
       "       101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113,\n",
       "       114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 124, 125,\n",
       "       126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "       139, 140, 141, 142, 143, 144])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_fp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
