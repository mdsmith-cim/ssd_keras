{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD300 Inference Tutorial\n",
    "\n",
    "This is a brief tutorial that shows how to use a trained SSD300 for inference on the Pascal VOC datasets. If you'd like more detailed explanations, please refer to [`ssd300_training.ipynb`](https://github.com/pierluigiferrari/ssd_keras/blob/master/ssd300_training.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from scipy.stats import entropy\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import os.path as p\n",
    "import glob\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from itertools import compress\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Model\n",
    "from ssd_encoder_decoder.ssd_output_decoder_dropout import decode_detections\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "from data_generator.object_detection_2d_image_boxes_validation_utils import BoxFilter\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "\n",
    "from bounding_box_utils.bounding_box_utils import iou\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image size.\n",
    "img_height = 300\n",
    "img_width = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a trained SSD\n",
    "\n",
    "Either load a trained model or build a model and load trained weights into it. Since the HDF5 files I'm providing contain only the weights for the various SSD versions, not the complete models, you'll have to go with the latter option when using this implementation for the first time. You can then of course save the model and next time load the full model directly, without having to build it.\n",
    "\n",
    "You can find the download links to all the trained model weights in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Build the model and load trained weights into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 20 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True\n",
    "n_boxes = 8732\n",
    "N = 10 # Number of passes through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                swap_channels=swap_channels,\n",
    "               dropout_rate=0.5)\n",
    "\n",
    "\n",
    "# 2: Load the trained weights into the model.\n",
    "\n",
    "# TODO: Set the path of the trained weights.\n",
    "weights_path = 'good_dropout_model/ssd300_dropout_PASCAL2012_train_+12_epoch-58_loss-3.8960_val_loss-5.0832.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Compile the model so that Keras won't complain the next time you load it.\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load some images\n",
    "\n",
    "Load some images for which you'd like the model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = '/usr/local/data/msmith/APL/Datasets/KITTI_converted/'\n",
    "# The directories that contain the images.\n",
    "VOC_2012_images_dir      = p.join(ROOT_PATH,'VOC2012/JPEGImages/')\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "VOC_2012_annotations_dir      = p.join(ROOT_PATH,'VOC2012/Annotations/')\n",
    "\n",
    "# The paths to the image sets.\n",
    "VOC_2012_trainval_image_set_filename = p.join(ROOT_PATH,'VOC2012/ImageSets/Main/trainval.txt')\n",
    "\n",
    "# KITTI_classes = ['car', 'Van', 'Truck', 'Pedestrian', 'person', 'Cyclist', 'Tram','Misc', 'DontCare']\n",
    "# PASCAL_classes = ['background',\n",
    "#            'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "#            'bottle', 'bus', 'car', 'cat',\n",
    "#            'chair', 'cow', 'diningtable', 'dog',\n",
    "#            'horse', 'motorbike', 'person', 'pottedplant',\n",
    "#            'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "classes = ['background',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat',\n",
    "           'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "dataset = DataGenerator(load_images_into_memory=False)\n",
    "dataset.parse_xml(images_dirs=[VOC_2012_images_dir],\n",
    "                  image_set_filenames=[VOC_2012_trainval_image_set_filename],\n",
    "                  annotations_dirs=[VOC_2012_annotations_dir],\n",
    "                  classes=classes,\n",
    "                  include_classes='all',\n",
    "                  exclude_truncated=False,\n",
    "                  exclude_difficult=False,\n",
    "                  ret=False,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine resize because default version has weird behaviour\n",
    "class Resize:\n",
    "    '''\n",
    "    Resizes images to a specified height and width in pixels.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 height,\n",
    "                 width,\n",
    "                 interpolation_mode=cv2.INTER_LINEAR,\n",
    "                 box_filter=None,\n",
    "                 labels_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}):\n",
    "        '''\n",
    "        Arguments:\n",
    "            height (int): The desired height of the output images in pixels.\n",
    "            width (int): The desired width of the output images in pixels.\n",
    "            interpolation_mode (int, optional): An integer that denotes a valid\n",
    "                OpenCV interpolation mode. For example, integers 0 through 5 are\n",
    "                valid interpolation modes.\n",
    "            box_filter (BoxFilter, optional): Only relevant if ground truth bounding boxes are given.\n",
    "                A `BoxFilter` object to filter out bounding boxes that don't meet the given criteria\n",
    "                after the transformation. Refer to the `BoxFilter` documentation for details. If `None`,\n",
    "                the validity of the bounding boxes is not checked.\n",
    "            labels_format (dict, optional): A dictionary that defines which index in the last axis of the labels\n",
    "                of an image contains which bounding box coordinate. The dictionary maps at least the keywords\n",
    "                'xmin', 'ymin', 'xmax', and 'ymax' to their respective indices within last axis of the labels array.\n",
    "        '''\n",
    "        if not (isinstance(box_filter, BoxFilter) or box_filter is None):\n",
    "            raise ValueError(\"`box_filter` must be either `None` or a `BoxFilter` object.\")\n",
    "        self.out_height = height\n",
    "        self.out_width = width\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "        self.box_filter = box_filter\n",
    "        self.labels_format = labels_format\n",
    "\n",
    "    def __call__(self, image, labels=None, return_inverter=False):\n",
    "\n",
    "        img_height, img_width = image.shape[:2]\n",
    "\n",
    "        xmin = self.labels_format['xmin']\n",
    "        ymin = self.labels_format['ymin']\n",
    "        xmax = self.labels_format['xmax']\n",
    "        ymax = self.labels_format['ymax']\n",
    "\n",
    "        image = cv2.resize(image,\n",
    "                           dsize=(self.out_width, self.out_height),\n",
    "                           interpolation=self.interpolation_mode)\n",
    "\n",
    "        if return_inverter:\n",
    "            def inverter(labels):\n",
    "                labels = np.copy(labels)\n",
    "                labels[:, [ymin, ymax]] = np.round(labels[:, [ymin, ymax]] * (img_height / self.out_height), decimals=0)\n",
    "                labels[:, [xmin, xmax]] = np.round(labels[:, [xmin, xmax]] * (img_width / self.out_width), decimals=0)\n",
    "                return labels\n",
    "\n",
    "        if labels is None:\n",
    "            if return_inverter:\n",
    "                return image, inverter\n",
    "            else:\n",
    "                return image\n",
    "        else:\n",
    "            labels = np.copy(labels)\n",
    "            labels[:, [ymin, ymax]] = np.round(labels[:, [ymin, ymax]] * (self.out_height / img_height), decimals=0)\n",
    "            labels[:, [xmin, xmax]] = np.round(labels[:, [xmin, xmax]] * (self.out_width / img_width), decimals=0)\n",
    "\n",
    "            if not (self.box_filter is None):\n",
    "                self.box_filter.labels_format = self.labels_format\n",
    "                labels = self.box_filter(labels=labels,\n",
    "                                         image_height=self.out_height,\n",
    "                                         image_width=self.out_width)\n",
    "\n",
    "            if return_inverter:\n",
    "                return image, labels, inverter\n",
    "            else:\n",
    "                return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height,width=img_width, labels_format={'xmin': -4, 'ymin': -3, 'xmax': -2, 'ymax': -1})\n",
    "\n",
    "transformations = [convert_to_3_channels, resize]\n",
    "\n",
    "batch_size = 128\n",
    "generator = dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         transformations=transformations,\n",
    "                                         label_encoder=None,\n",
    "                                         returns={'original_images',\n",
    "                                                  'processed_images',\n",
    "                                                  'image_ids',\n",
    "                                                  'evaluation-neutral',\n",
    "                                                  'inverse_transform',\n",
    "                                                  'original_labels'},\n",
    "                                         keep_images_without_gt=True,\n",
    "                                         degenerate_box_handling='remove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = dataset.get_dataset_size()\n",
    "# n_images = 10*batch_size\n",
    "n_batches = int(np.ceil(n_images / batch_size))\n",
    "# n_batches = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/usr/local/data/msmith/APL/dropout_time_series_rawsample'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Set the colors for the bounding boxes\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "# classes = ['background',\n",
    "#            'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "#            'bottle', 'bus', 'car', 'cat',\n",
    "#            'chair', 'cow', 'diningtable', 'dog',\n",
    "#            'horse', 'motorbike', 'person', 'pottedplant',\n",
    "#            'sheep', 'sofa', 'train', 'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coordinates(tensor, start_index, conversion, border_pixels='half'):\n",
    "    '''\n",
    "    Convert coordinates for axis-aligned 2D boxes between two coordinate formats.\n",
    "\n",
    "    Creates a copy of `tensor`, i.e. does not operate in place. Currently there are\n",
    "    three supported coordinate formats that can be converted from and to each other:\n",
    "        1) (xmin, xmax, ymin, ymax) - the 'minmax' format\n",
    "        2) (xmin, ymin, xmax, ymax) - the 'corners' format\n",
    "        2) (cx, cy, w, h) - the 'centroids' format\n",
    "\n",
    "    Arguments:\n",
    "        tensor (array): A Numpy nD array containing the four consecutive coordinates\n",
    "            to be converted somewhere in the last axis.\n",
    "        start_index (int): The index of the first coordinate in the last axis of `tensor`.\n",
    "        conversion (str, optional): The conversion direction. Can be 'minmax2centroids',\n",
    "            'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners',\n",
    "            or 'corners2minmax'.\n",
    "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
    "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
    "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
    "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
    "            to the boxex, but not the other.\n",
    "\n",
    "    Returns:\n",
    "        A Numpy nD array, a copy of the input tensor with the converted coordinates\n",
    "        in place of the original coordinates and the unaltered elements of the original\n",
    "        tensor elsewhere.\n",
    "    '''\n",
    "    if border_pixels == 'half':\n",
    "        d = 0\n",
    "    elif border_pixels == 'include':\n",
    "        d = 1\n",
    "    elif border_pixels == 'exclude':\n",
    "        d = -1\n",
    "\n",
    "    ind = start_index\n",
    "    tensor1 = np.copy(tensor).astype(np.float)\n",
    "    if conversion == 'minmax2centroids':\n",
    "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+1]) / 2.0 # Set cx\n",
    "        tensor1[..., ind+1] = (tensor[..., ind+2] + tensor[..., ind+3]) / 2.0 # Set cy\n",
    "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind] + d # Set w\n",
    "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+2] + d # Set h\n",
    "    elif conversion == 'centroids2minmax':\n",
    "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
    "        tensor1[..., ind+1] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
    "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
    "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
    "    elif conversion == 'corners2centroids':\n",
    "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+2]) / 2.0 # Set cx\n",
    "        tensor1[..., ind+1] = (tensor[..., ind+1] + tensor[..., ind+3]) / 2.0 # Set cy\n",
    "        tensor1[..., ind+2] = tensor[..., ind+2] - tensor[..., ind] + d # Set w\n",
    "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+1] + d # Set h\n",
    "    elif conversion == 'centroids2corners':\n",
    "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
    "        tensor1[..., ind+1] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
    "        tensor1[..., ind+2] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
    "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
    "    elif (conversion == 'minmax2corners') or (conversion == 'corners2minmax'):\n",
    "        tensor1[..., ind+1] = tensor[..., ind+2]\n",
    "        tensor1[..., ind+2] = tensor[..., ind+1]\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected conversion value. Supported values are 'minmax2centroids', 'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners', and 'corners2minmax'.\")\n",
    "\n",
    "    return tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGPUPredictions(generator, n_batches, N):\n",
    "#     output = []\n",
    "    for _ in tqdm(range(n_batches), desc='Batch'):\n",
    "        batch_X, batch_image_ids, batch_eval_neutral, batch_inverse_transforms, original_X, batch_orig_labels = next(generator)\n",
    "        \n",
    "        model_out = np.empty((N, batch_size, model.output_shape[1], model.output_shape[2]), dtype=np.float32)\n",
    "        for n in tqdm(range(N), leave=False, desc='MC Dropout'):\n",
    "            model_out[n] = model.predict(batch_X)\n",
    "\n",
    "        # Model output: (batch, number of boxes, 33)\n",
    "        # Last axis: num classes softmax (21), object locations (4), anchor boxes position, size and variances (8)\n",
    "\n",
    "        y_pred_decoded_raw = np.copy(model_out[:, :, :,:-8]) # Slice out the classes and the four offsets, throw away the anchor coordinates and variances, resulting in a tensor of shape `[runs, batch, n_boxes, n_classes + 4 coordinates]`\n",
    "\n",
    "        y_pred_decoded_raw[:, :, :, [-2,-1]] = np.exp(y_pred_decoded_raw[:, :, :, [-2,-1]] * model_out[:, :, :,[-2,-1]]) # exp(ln(w(pred)/w(anchor)) / w_variance * w_variance) == w(pred) / w(anchor), exp(ln(h(pred)/h(anchor)) / h_variance * h_variance) == h(pred) / h(anchor)\n",
    "        y_pred_decoded_raw[:, :, :, [-2,-1]] *= model_out[:, :, :, [-6,-5]] # (w(pred) / w(anchor)) * w(anchor) == w(pred), (h(pred) / h(anchor)) * h(anchor) == h(pred)\n",
    "        y_pred_decoded_raw[:, :, :, [-4,-3]] *= model_out[:, :, :, [-4,-3]] * model_out[:, :, :, [-6,-5]] # (delta_cx(pred) / w(anchor) / cx_variance) * cx_variance * w(anchor) == delta_cx(pred), (delta_cy(pred) / h(anchor) / cy_variance) * cy_variance * h(anchor) == delta_cy(pred)\n",
    "        y_pred_decoded_raw[:, :, :, [-4,-3]] += model_out[:, :, :, [-8,-7]] # delta_cx(pred) + cx(anchor) == cx(pred), delta_cy(pred) + cy(anchor) == cy(pred)\n",
    "        y_pred_decoded_raw = convert_coordinates(y_pred_decoded_raw, start_index=-4, conversion='centroids2corners')\n",
    "\n",
    "        y_pred_decoded_raw[:, :, :, [-4,-2]] *= img_width # Convert xmin, xmax back to absolute coordinates\n",
    "        y_pred_decoded_raw[:, :, :, [-3,-1]] *= img_height # Convert ymin, ymax back to absolute coordinates\n",
    "        \n",
    "        # Iterate over all images\n",
    "        for img in y_pred_decoded_raw:\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Indexing : [image #][observation #]\n",
    "        # Note that each image will have different numbers of observations\n",
    "        # Each observation is an array with shape (num_detections, num_class+6)\n",
    "        observations_per_img = []\n",
    "\n",
    "        filtered = [c for c in model_out[:,i] if 0 not in c.shape]\n",
    "        if len(filtered) == 0:\n",
    "            concated = np.empty((0))\n",
    "        else:\n",
    "            concated =  np.concatenate(filtered)\n",
    "        # Concated is now an array of shape (total num detections, num_class + 6)\n",
    "        # Note that there will be many overlaps; we are now trying to remove them\n",
    "\n",
    "        \n",
    "        \n",
    "        observations = []\n",
    "        while concated.shape[0] > 0:\n",
    "            # Get first bounding box\n",
    "            box = concated[0, :]\n",
    "            # Calculate IoU for between said first box an all others\n",
    "            ious = iou(concated[:, -4:], box[-4:], coords='corners', border_pixels='half', mode='element-wise')\n",
    "            # Get matches, guaranteed one match min. to itself\n",
    "            keep_idx = ious >= iou_threshold\n",
    "            # Add box to list of boxes\n",
    "            # This way we are grouping all overlapping boxes\n",
    "            observations.append(concated[keep_idx, :])\n",
    "            # Remove the boxes we just found were overlapping (min. itself) and keep going\n",
    "            concated = concated[np.invert(keep_idx),:]\n",
    "        observations_per_img.append(observations)\n",
    "        \n",
    "#         output.append((batch_X, batch_image_ids, batch_eval_neutral, batch_inverse_transforms, original_X, batch_orig_labels, y_pred_decoded_raw))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertBatchesToImages(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runInference(gpuOut, iou_threshold=0.95):\n",
    "    results = []\n",
    "    for b in tqdm(range(n_batches)):\n",
    "        batch_X, batch_image_ids, batch_eval_neutral, batch_inverse_transforms, original_X, batch_orig_labels, model_out = gpuOut[b]\n",
    "        bs = len(batch_X)\n",
    "\n",
    "        # model_out is an array (Num passes, batch size)\n",
    "\n",
    "        # Indexing : [image #][observation #]\n",
    "        # Note that each image will have different numbers of observations\n",
    "        # Each observation is an array with shape (num_detections, num_class+6)\n",
    "        observations_per_img = []\n",
    "\n",
    "        # Iterate over each batch image\n",
    "        for i in range(bs):\n",
    "            filtered = [c for c in model_out[:,i] if 0 not in c.shape]\n",
    "            if len(filtered) == 0:\n",
    "                concated = np.empty((0))\n",
    "            else:\n",
    "                concated =  np.concatenate(filtered)\n",
    "            # Concated is now an array of shape (total num detections, num_class + 6)\n",
    "            # Note that there will be many overlaps; we are now trying to remove them\n",
    "\n",
    "            observations = []\n",
    "            while concated.shape[0] > 0:\n",
    "                # Get first bounding box\n",
    "                box = concated[0, :]\n",
    "                # Calculate IoU for between said first box an all others\n",
    "                ious = iou(concated[:, -4:], box[-4:], coords='corners', border_pixels='half', mode='element-wise')\n",
    "                # Get matches, guaranteed one match min. to itself\n",
    "                keep_idx = ious >= iou_threshold\n",
    "                # Add box to list of boxes\n",
    "                # This way we are grouping all overlapping boxes\n",
    "                observations.append(concated[keep_idx, :])\n",
    "                # Remove the boxes we just found were overlapping (min. itself) and keep going\n",
    "                concated = concated[np.invert(keep_idx),:]\n",
    "            observations_per_img.append(observations)\n",
    "\n",
    "        # Get label probabilities\n",
    "#         observations_decoded_per_img = []\n",
    "#         for i in range(bs):\n",
    "#             observations_decoded = []\n",
    "#             for obs in observations_per_img[i]:\n",
    "#                 # in observation: index 0-class id of max, 1-confidence of said class, 2-23 inclusive: softmax values, 24-27: box coordinates\n",
    "#                 means = np.mean(obs[:,2:], axis=0) # Get mean values of softmax and boxes per paper\n",
    "#                 ent = entropy(means[:-4]) # Get uncertainty estimate using entropy of mean (total uncertainty)\n",
    "#                 # Max entropy of 21 values slightly greater than 3\n",
    "#                 # Min is 0 obviously\n",
    "#                 new_class = np.argmax(means[1:-4]) + 1 # Get max class (ignoring background)\n",
    "#                 new_obs = np.empty((7))\n",
    "#                 new_obs[0:3] = [new_class, means[new_class] , ent]\n",
    "#                 new_obs[3:7] = means[-4:]\n",
    "#                 # New format: class id, associated class softmax confidence, entropy value, bounding box values\n",
    "#                 observations_decoded.append(new_obs)\n",
    "#             observations_decoded = np.array(observations_decoded)\n",
    "#             # If not empty\n",
    "#             if len(observations_decoded) > 0:\n",
    "#                 observations_decoded = observations_decoded[observations_decoded[:,1].argsort()]\n",
    "#             observations_decoded_per_img.append(observations_decoded)\n",
    "#             # Index format: [img #][observation #]\n",
    "\n",
    "#         # Now transformed to original bounding box coord for display\n",
    "#         # Index: [img #][detection #](class id, associated class softmax confidence, entropy value, bounding box values)\n",
    "#         observations_decoded_per_img = apply_inverse_transforms(observations_decoded_per_img, batch_inverse_transforms)\n",
    "        \n",
    "        # Save data for later processing\n",
    "        results.append((observations_per_img, batch_image_ids, batch_orig_labels, batch_inverse_transforms, original_X))\n",
    "        \n",
    "#         if saveFigures:\n",
    "#             # Disable showing plot\n",
    "#             plt.ioff()\n",
    "#             #plt.ion()\n",
    "\n",
    "#             # Display the image and draw the predicted boxes onto it.\n",
    "#             for i in range(bs):\n",
    "#                 observations_decoded = observations_decoded_per_img[i]\n",
    "\n",
    "#                 save_file = Path(os.path.join(save_dir, batch_image_ids[i] + '.pdf'))\n",
    "#                 save_file_jpg = Path(os.path.join(save_dir, batch_image_ids[i] + '.jpg'))\n",
    "#                 if not save_file.exists():\n",
    "#                     plt.figure(figsize=(20,12))\n",
    "#                     plt.imshow(original_X[i])\n",
    "#                     plt.axis('off')\n",
    "\n",
    "#                     current_axis = plt.gca()\n",
    "\n",
    "#                     for observation in observations_decoded:\n",
    "#                         # Transform the predicted bounding boxes for the 300x300 image to the original image dimensions.\n",
    "#                         xmin = observation[-4]\n",
    "#                         ymin = observation[-3]\n",
    "#                         xmax = observation[-2]\n",
    "#                         ymax = observation[-1]\n",
    "#                         color = colors[int(observation[0])]\n",
    "#                         label = '{}: {:.2f} | {:.2f}'.format(classes[int(observation[0])], observation[1], observation[2])\n",
    "#                         current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "#                         current_axis.text(xmin, ymin, label, size='medium', color='white', bbox={'facecolor':color, 'alpha':1.0})\n",
    "\n",
    "#                     plt.savefig(save_file, bbox_inches='tight', pad_inches=0)\n",
    "#                     plt.savefig(save_file_jpg, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "#                     plt.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpuOut = runGPUPredictions(generator, n_batches, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = runInference(gpuOut, 0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert output to more usable format indexed by image number\n",
    "prediction_list = []\n",
    "id_list = []\n",
    "gt_list = []\n",
    "inv_transform_list = []\n",
    "original_X_list = []\n",
    "for i in results:\n",
    "    prediction_list.extend(i[0])\n",
    "    id_list.extend(i[1])\n",
    "    gt_list.extend(i[2])\n",
    "    inv_transform_list.extend(i[3])\n",
    "    original_X_list.extend(i[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTimeSeriesAccumulation(predictionList, idList, iou_threshold=0.5, maxMisses=3):\n",
    "    \n",
    "    # Create samples for background class = no detection\n",
    "    zeroSamples = np.zeros((N, n_classes + 1))\n",
    "    zeroSamples[:, 0] = 1.0\n",
    "    \n",
    "    numFramesTotal = len(predictionList)\n",
    "    \n",
    "    # track objects in this image\n",
    "    imageState = []\n",
    "    # image states, for each frame\n",
    "    imageStateAccum = []\n",
    "    \n",
    "    # Iterate over frames\n",
    "    for i in tqdm(range(numFramesTotal)):\n",
    "    \n",
    "        # Keep image state from previous frame to aggregate data\n",
    "        oldImageState = np.array(imageState)\n",
    "        imageState = []\n",
    "        \n",
    "        # If this is a new sequence, discard previous state\n",
    "        currentID = idList[i]\n",
    "        if int(currentID.split('-')[1]) == 0:\n",
    "            oldImageState = np.empty((0,))\n",
    "\n",
    "        currentPredictions = np.array(predictionList[i])\n",
    "\n",
    "        # Process all observations for current frame\n",
    "        for obs in currentPredictions:\n",
    "            meanbbox = np.mean(obs[:, -4:], axis=0)\n",
    "\n",
    "            # Check for matching observations in previous frame(s) via IoU intersection\n",
    "            stateMeanBoxes = []\n",
    "            for sObs in oldImageState:\n",
    "                stateMeanBoxes.append(np.mean(sObs['bbox'], axis=0))\n",
    "            stateMeanBoxes = np.array(stateMeanBoxes)\n",
    "\n",
    "            if len(stateMeanBoxes) > 0:\n",
    "                ious = iou(stateMeanBoxes, meanbbox, coords='corners', border_pixels='half', mode='element-wise')\n",
    "                matchesS = ious > iou_threshold\n",
    "            else:\n",
    "                matchesS = []\n",
    "\n",
    "            stateMatches = oldImageState[matchesS]\n",
    "            \n",
    "            # Get softmax samples from current frame\n",
    "            softmax = [obs[:, 2:-4]]\n",
    "            \n",
    "            # Add in data from previous observations\n",
    "            # In case there are no previous observations, this does nothing\n",
    "            for m in stateMatches:\n",
    "                softmax.extend(m['softmax'])\n",
    "                \n",
    "            # Delete old observations now that we have used their data\n",
    "            if np.any(matchesS):\n",
    "                oldImageState = oldImageState[np.invert(matchesS)]\n",
    "                \n",
    "            # Create new observation for our current state\n",
    "            # in observation: index 0-class id of max, 1-confidence of said class, 2-23 inclusive: softmax values, 24-27: box coordinates\n",
    "            # Super observation bbox samples are from current frame (object may have moved), softmax from all\n",
    "            superObservation = {'bbox': obs[:, -4:], 'softmax' : softmax, 'missedDetections': 0}\n",
    "\n",
    "            imageState.append(superObservation)\n",
    "        \n",
    "        # Process remaining unmatched previous states\n",
    "        for obs in oldImageState:\n",
    "\n",
    "            # If too many missed detections: ignore old state and let it be forgotten\n",
    "            if obs['missedDetections'] < maxMisses:\n",
    "\n",
    "                # At this point, we have no detection in this frame\n",
    "                # i.e. all N dropout runs produced no detection above the low threshold\n",
    "                # Thus we add a bunch of samples with softmax values for background\n",
    "                # Old bounding box is reused, although it may be off\n",
    "                softmax = [zeroSamples]\n",
    "                softmax.extend(obs['softmax'])\n",
    "                superObservation = {'bbox': obs['bbox'], 'softmax' : softmax, 'missedDetections': obs['missedDetections'] + 1}\n",
    "\n",
    "                imageState.append(superObservation)\n",
    "        \n",
    "        #imageState now = complete list of observations for current image taking into account past frame(s)\n",
    "        imageStateAccum.append(imageState)\n",
    "        \n",
    "        \n",
    "    return imageStateAccum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "timeSeriesAccum = runTimeSeriesAccumulation(prediction_list, id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFigures(imageStates, idList, gtList, batchTransforms, originalX, softmax_threshold=0.1):\n",
    "\n",
    "    for idx in tqdm(range(0, len(imageStates))):\n",
    "    \n",
    "        imageState = imageStates[idx]\n",
    "        imageId = idList[idx]\n",
    "        gt = gtList[idx]\n",
    "        inverters = batchTransforms[idx]\n",
    "        originalImage = originalX[idx]\n",
    "        \n",
    "        save_file = Path(os.path.join(save_dir, imageId + '.pdf'))\n",
    "        save_file_jpg = Path(os.path.join(save_dir, imageId + '.jpg'))\n",
    "        \n",
    "        plt.ioff()\n",
    "        plt.figure(figsize=(20,12))\n",
    "        plt.imshow(originalImage)\n",
    "        plt.axis('off')\n",
    "        current_axis = plt.gca()\n",
    "        \n",
    "        for state in imageState:\n",
    "\n",
    "            mean_bbox = np.mean(state['bbox'], axis=0)\n",
    "            \n",
    "            # TODO:\n",
    "            # Can do some weighting here -> treat more recent samples as more important\n",
    "            # Should we be using entropy only on non-background classes?\n",
    "            # Getting real samples from the network output...\n",
    "    \n",
    "            allSamples = np.concatenate(state['softmax'], axis=0)\n",
    "            \n",
    "#             if state['missedDetections'] > 0:\n",
    "#                 set_trace()\n",
    "        \n",
    "            sample_mean = np.mean(allSamples, axis=0)\n",
    "\n",
    "            class_id = np.argmax(sample_mean)\n",
    "            class_id_no_background = np.argmax(sample_mean[1:]) + 1\n",
    "            \n",
    "#             max_class_softmax_value = sample_mean[class_id]\n",
    "            max_class_softmax_value_no_bkg = sample_mean[class_id_no_background]\n",
    "    \n",
    "            # Ignore predictions with very low confidence and don't draw them\n",
    "            if max_class_softmax_value_no_bkg < softmax_threshold:\n",
    "                continue\n",
    "\n",
    "#             class_entropy = entropy(sample_mean)\n",
    "            sample_entropy = entropy(allSamples[:, class_id_no_background])\n",
    "\n",
    "            class_entropy_no_bkg = entropy(sample_mean[1:])\n",
    "\n",
    "            # Apply inverse transforms\n",
    "            for inverter in inverters:\n",
    "                if not (inverter is None):\n",
    "                    mean_bbox = inverter(np.expand_dims(mean_bbox, axis=0)).squeeze()\n",
    "\n",
    "            xmin = mean_bbox[-4]\n",
    "            ymin = mean_bbox[-3]\n",
    "            xmax = mean_bbox[-2]\n",
    "            ymax = mean_bbox[-1]\n",
    "            color = colors[int(class_id_no_background)]\n",
    "            \n",
    "            # Background class is dominant:\n",
    "            if class_id != class_id_no_background:\n",
    "                linestyle = 'dashed'\n",
    "                box_spec = {'facecolor':color, 'alpha':0.5}\n",
    "                if state['missedDetections'] > 0:\n",
    "                    linestyle = 'dashdot'\n",
    "            else:\n",
    "                linestyle = 'solid'\n",
    "                box_spec = {'facecolor':color, 'alpha':0.85}\n",
    "                if state['missedDetections'] > 0:\n",
    "                    linestyle = 'dotted'\n",
    "                \n",
    "            current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2, linestyle=linestyle))  \n",
    "            label = '{}: {:.2f} | {:.2f} | {:.2f}'.format(classes[int(class_id_no_background)], max_class_softmax_value_no_bkg, class_entropy_no_bkg, sample_entropy)\n",
    "            current_axis.text(xmin, ymin, label, size='medium', color='white', bbox=box_spec)\n",
    "\n",
    "        # Ignore PDF save to speed up process\n",
    "        #plt.savefig(save_file, bbox_inches='tight', pad_inches=0)\n",
    "        plt.savefig(save_file_jpg, bbox_inches='tight', pad_inches=0, optimize=True)\n",
    "\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saveFigures(timeSeriesAccum, id_list, gt_list, inv_transform_list, original_X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gt_format={'class_id': 0, 'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}\n",
    "def get_num_gt_per_class(verbose=True):\n",
    "    '''\n",
    "    Counts the number of ground truth boxes for each class across the dataset.\n",
    "\n",
    "    Arguments:\n",
    "        verbose (bool, optional): If `True`, will print out the progress during runtime.\n",
    "\n",
    "    Returns:\n",
    "        A list containing a count of the number of ground truth boxes for each class across the\n",
    "        entire dataset.\n",
    "    '''\n",
    "\n",
    "    num_gt_per_class = np.zeros(shape=(n_classes+1), dtype=np.int)\n",
    "\n",
    "    # Iterate over the ground truth for all images in the dataset.\n",
    "    for i in tqdm(range(len(gt_list))):\n",
    "\n",
    "        boxes = np.asarray(gt_list[i])\n",
    "\n",
    "        # Iterate over all ground truth boxes for the current image.\n",
    "        for j in range(boxes.shape[0]):\n",
    "            class_id = boxes[j, 0]\n",
    "            num_gt_per_class[class_id] += 1\n",
    "\n",
    "    return num_gt_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gt_per_class = get_num_gt_per_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumFalseNegatives(num_gt_per_class, tp_vector):\n",
    "    tp_per_class = []\n",
    "    for tp in tp_vector:\n",
    "        tp_per_class.append(np.array(tp).sum())\n",
    "    \n",
    "    tp_per_class = np.array(tp_per_class)\n",
    "    \n",
    "    return num_gt_per_class - tp_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_predictions(prediction_list, id_list, gt_list, \n",
    "                      matching_iou_threshold=0.5,\n",
    "                      border_pixels='include',\n",
    "                      sorting_algorithm='quicksort'):\n",
    "    '''\n",
    "    Matches predictions to ground truth boxes.\n",
    "\n",
    "    Note that `predict_on_dataset()` must be called before calling this method.\n",
    "\n",
    "    Arguments:\n",
    "        matching_iou_threshold (float, optional): A prediction will be considered a true positive if it has a Jaccard overlap\n",
    "            of at least `matching_iou_threshold` with any ground truth bounding box of the same class.\n",
    "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
    "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
    "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
    "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
    "            to the boxex, but not the other.\n",
    "        sorting_algorithm (str, optional): Which sorting algorithm the matching algorithm should use. This argument accepts\n",
    "            any valid sorting algorithm for Numpy's `argsort()` function. You will usually want to choose between 'quicksort'\n",
    "            (fastest and most memory efficient, but not stable) and 'mergesort' (slight slower and less memory efficient, but stable).\n",
    "            The official Matlab evaluation algorithm uses a stable sorting algorithm, so this algorithm is only guaranteed\n",
    "            to behave identically if you choose 'mergesort' as the sorting algorithm, but it will almost always behave identically\n",
    "            even if you choose 'quicksort' (but no guarantees).\n",
    "\n",
    "    Returns:\n",
    "        Four nested lists containing the true positives, false positives, cumulative true positives,\n",
    "        and cumulative false positives for each class.\n",
    "    '''\n",
    "\n",
    "    # Convert the ground truth to a more efficient format for what we need\n",
    "    # to do, which is access ground truth by image ID repeatedly.\n",
    "    ground_truth = {}\n",
    "    n_images = len(prediction_list)\n",
    "    for i in range(n_images):\n",
    "        image_id = str(id_list[i])\n",
    "        labels = gt_list[i]\n",
    "        ground_truth[image_id] = np.asarray(labels)\n",
    "\n",
    "    true_positives = [[]] # The false positives for each class, sorted by descending confidence.\n",
    "    false_positives = [[]] # The true positives for each class, sorted by descending confidence.\n",
    "#     cumulative_true_positives = [[]]\n",
    "#     cumulative_false_positives = [[]]\n",
    "\n",
    "    # Iterate over all images\n",
    "    predictions_by_class = [list() for _ in range(n_classes + 1)]\n",
    "    for k, prediction in enumerate(prediction_list):\n",
    "\n",
    "        image_id = id_list[k]\n",
    "\n",
    "        for box in prediction:\n",
    "            class_id = int(box[0])\n",
    "            confidence = box[1]\n",
    "            ent = box[2]\n",
    "            xmin = round(box[-4], 1)\n",
    "            ymin = round(box[-3], 1)\n",
    "            xmax = round(box[-2], 1)\n",
    "            ymax = round(box[-1], 1)\n",
    "            prediction = (image_id, confidence, ent, xmin, ymin, xmax, ymax)\n",
    "            # Append the predicted box to the results list for its class.\n",
    "            predictions_by_class[class_id].append(prediction)\n",
    "    \n",
    "    # Iterate over all classes.\n",
    "    for class_id in range(1, n_classes + 1):\n",
    "\n",
    "        predict_by_class = predictions_by_class[class_id]\n",
    "\n",
    "        # Store the matching results in these lists:\n",
    "        true_pos = np.zeros(len(predict_by_class), dtype=np.int) # 1 for every prediction that is a true positive, 0 otherwise\n",
    "        false_pos = np.zeros(len(predict_by_class), dtype=np.int) # 1 for every prediction that is a false positive, 0 otherwise\n",
    "\n",
    "        # In case there are no predictions at all for this class, we're done here.\n",
    "        if len(predict_by_class) == 0:\n",
    "            print(\"No predictions for class {}/{}\".format(class_id, n_classes))\n",
    "            true_positives.append(true_pos)\n",
    "            false_positives.append(false_pos)\n",
    "            continue\n",
    "\n",
    "        # Convert the predictions list for this class into a structured array so that we can sort it by confidence.\n",
    "\n",
    "        # Get the number of characters needed to store the image ID strings in the structured array.\n",
    "        num_chars_per_image_id = len(str(predict_by_class[0][0])) + 6 # Keep a few characters buffer in case some image IDs are longer than others.\n",
    "        # Create the data type for the structured array.\n",
    "        preds_data_type = np.dtype([('image_id', 'U{}'.format(num_chars_per_image_id)),\n",
    "                                    ('confidence', 'f4'),\n",
    "                                    ('entropy', 'f4'),\n",
    "                                    ('xmin', 'f4'),\n",
    "                                    ('ymin', 'f4'),\n",
    "                                    ('xmax', 'f4'),\n",
    "                                    ('ymax', 'f4')])\n",
    "        # Create the structured array\n",
    "        predict_by_class = np.array(predict_by_class, dtype=preds_data_type)\n",
    "\n",
    "        # Sort the detections by decreasing confidence.\n",
    "        descending_indices = np.argsort(-predict_by_class['confidence'], kind=sorting_algorithm)\n",
    "        predictions_sorted = predict_by_class[descending_indices]\n",
    "\n",
    "        # Keep track of which ground truth boxes were already matched to a detection.\n",
    "        gt_matched = {}\n",
    "\n",
    "        # Iterate over all predictions.\n",
    "        for i in range(len(predict_by_class)):\n",
    "\n",
    "            prediction = predictions_sorted[i]\n",
    "            image_id = prediction['image_id']\n",
    "            pred_box = np.asarray(list(prediction[['xmin', 'ymin', 'xmax', 'ymax']])) # Convert the structured array element to a regular array.\n",
    "\n",
    "            # Get the relevant ground truth boxes for this prediction,\n",
    "            # i.e. all ground truth boxes that match the prediction's\n",
    "            # image ID and class ID.\n",
    "\n",
    "            gt = ground_truth[image_id]\n",
    "            gt = np.asarray(gt)\n",
    "            class_mask = gt[:,0] == class_id\n",
    "            gt = gt[class_mask]\n",
    "\n",
    "            if gt.size == 0:\n",
    "                # If the image doesn't contain any objects of this class,\n",
    "                # the prediction becomes a false positive.\n",
    "                false_pos[i] = 1\n",
    "                continue\n",
    "\n",
    "            # Compute the IoU of this prediction with all ground truth boxes of the same class.\n",
    "            overlaps = iou(boxes1=gt[:,[1, 2, 3, 4]],\n",
    "                           boxes2=pred_box,\n",
    "                           coords='corners',\n",
    "                           mode='element-wise',\n",
    "                           border_pixels=border_pixels)\n",
    "\n",
    "            # For each detection, match the ground truth box with the highest overlap.\n",
    "            # It's possible that the same ground truth box will be matched to multiple\n",
    "            # detections.\n",
    "            gt_match_index = np.argmax(overlaps)\n",
    "            gt_match_overlap = overlaps[gt_match_index]\n",
    "\n",
    "            if gt_match_overlap < matching_iou_threshold:\n",
    "                # False positive, IoU threshold violated:\n",
    "                # Those predictions whose matched overlap is below the threshold become\n",
    "                # false positives.\n",
    "                false_pos[i] = 1\n",
    "            else:\n",
    "                if not (image_id in gt_matched):\n",
    "                    # True positive:\n",
    "                    # If the matched ground truth box for this prediction hasn't been matched to a\n",
    "                    # different prediction already, we have a true positive.\n",
    "                    true_pos[i] = 1\n",
    "                    gt_matched[image_id] = np.zeros(shape=(gt.shape[0]), dtype=np.bool)\n",
    "                    gt_matched[image_id][gt_match_index] = True\n",
    "                elif not gt_matched[image_id][gt_match_index]:\n",
    "                    # True positive:\n",
    "                    # If the matched ground truth box for this prediction hasn't been matched to a\n",
    "                    # different prediction already, we have a true positive.\n",
    "                    true_pos[i] = 1\n",
    "                    gt_matched[image_id][gt_match_index] = True\n",
    "                else:\n",
    "                    # False positive, duplicate detection:\n",
    "                    # If the matched ground truth box for this prediction has already been matched\n",
    "                    # to a different prediction previously, it is a duplicate detection for an\n",
    "                    # already detected object, which counts as a false positive.\n",
    "                    false_pos[i] = 1\n",
    "\n",
    "        true_positives.append(true_pos)\n",
    "        false_positives.append(false_pos)\n",
    "\n",
    "#         cumulative_true_pos = np.cumsum(true_pos) # Cumulative sums of the true positives\n",
    "#         cumulative_false_pos = np.cumsum(false_pos) # Cumulative sums of the false positives\n",
    "\n",
    "#         cumulative_true_positives.append(cumulative_true_pos)\n",
    "#         cumulative_false_positives.append(cumulative_false_pos)\n",
    "\n",
    "    return true_positives, false_positives#, cumulative_true_positives, cumulative_false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns total number of false or true positives\n",
    "def get_total(fp_or_tp):\n",
    "    total = 0\n",
    "    for i in fp_or_tp:\n",
    "        total += np.array(i).sum()\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold predictions in different ways: either confidence or entropy\n",
    "# Removes any predictions less than the threshold\n",
    "def threshold_predictions(prediction_list, threshold_type='confidence', threshold=0.5):\n",
    "    if threshold_type == 'confidence':\n",
    "        index = 1\n",
    "        operator = np.greater_equal\n",
    "    elif threshold_type == 'entropy':\n",
    "        index = 2\n",
    "        operator = np.less_equal\n",
    "    else:\n",
    "        raise Exception('Unknown threshold type')\n",
    "        \n",
    "    new_img_list = []\n",
    "    for img_pred in prediction_list:\n",
    "        new_detection_list = []\n",
    "        for det in img_pred:\n",
    "            if operator(det[index], threshold):\n",
    "                new_detection_list.append(det)\n",
    "        new_img_list.append(np.array(new_detection_list))\n",
    "    \n",
    "    return new_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_confidence = np.linspace(0.01, 0.98, 100, endpoint=True)\n",
    "thresholds_entropy = np.linspace(0.01, 1.5, 100, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds_confidence = np.linspace(0.01, 0.98, 14, endpoint=True)\n",
    "# thresholds_entropy = np.linspace(0.01, 1.5, 14, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold_confidence_results = np.empty((len(thresholds_confidence), 3), dtype=np.int)\n",
    "for i, conf_thresh in tqdm(enumerate(thresholds_confidence), total=len(thresholds_confidence)):\n",
    "    new_pred_list = threshold_predictions(prediction_list2, 'confidence', conf_thresh)\n",
    "    tp, fp = match_predictions(new_pred_list, id_list2, gt_list2)\n",
    "    total_tp = get_total(tp)\n",
    "    total_fp = get_total(fp)\n",
    "    total_fn = getNumFalseNegatives(num_gt_per_class, tp).sum()\n",
    "    threshold_confidence_results[i, :] = [total_tp, total_fp, total_fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_entropy_results = np.empty((len(thresholds_entropy), 3), dtype=np.int)\n",
    "for i, ent_thresh in tqdm(enumerate(thresholds_entropy), total=len(thresholds_entropy)):\n",
    "    new_pred_list = threshold_predictions(prediction_list2, 'entropy', ent_thresh)\n",
    "    tp, fp = match_predictions(new_pred_list, id_list2, gt_list2)\n",
    "    total_tp = get_total(tp)\n",
    "    total_fp = get_total(fp)\n",
    "    total_fn = getNumFalseNegatives(num_gt_per_class, tp).sum()\n",
    "    threshold_entropy_results[i, :] = [total_tp, total_fp, total_fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax0 = plt.subplots(nrows=1,ncols=1, figsize=(10, 8))\n",
    "ax0.set(title='Confidence Thresholds', xlabel='Threshold', ylabel='Count')\n",
    "ax0.plot(thresholds_confidence, threshold_confidence_results[:,0], \"-\", label=\"%s\" % ('True Positives', ))\n",
    "ax0.plot(thresholds_confidence, threshold_confidence_results[:,1], \"-\", label=\"%s\" % ('False Positives', ))\n",
    "ax0.plot(thresholds_confidence, threshold_confidence_results[:,2], \"-\", label=\"%s\" % ('False Negatives', ))\n",
    "ax0.legend(loc=\"upper right\")\n",
    "\n",
    "fig.savefig('thresholds_conf_kitti_time.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax0 = plt.subplots(nrows=1,ncols=1, figsize=(10, 8))\n",
    "ax0.set(title='Entropy Thresholds', xlabel='Threshold', ylabel='Count')\n",
    "ax0.plot(thresholds_entropy, threshold_entropy_results[:,0], \"-\", label=\"%s\" % ('True Positives', ))\n",
    "ax0.plot(thresholds_entropy, threshold_entropy_results[:,1], \"-\", label=\"%s\" % ('False Positives', ))\n",
    "ax0.plot(thresholds_entropy, threshold_entropy_results[:,2], \"-\", label=\"%s\" % ('False Negatives', ))\n",
    "ax0.legend(loc=\"lower right\")\n",
    "\n",
    "fig.savefig('thresholds_entropy_kitti_time.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbestF1(threshold_results):\n",
    "    f1_scores = []\n",
    "    for i in threshold_results:\n",
    "        tp, fp, fn = i[0], i[1], i[2]\n",
    "        f1 = (2 * tp)/(2*tp + fp + fn)\n",
    "        f1_scores.append(f1)\n",
    "    return np.max(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Entropy F1: %f' % getbestF1(threshold_entropy_results))\n",
    "print('Best Conf F1: %f' % getbestF1(threshold_confidence_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_conf = []\n",
    "recall_conf = []\n",
    "for i in threshold_confidence_results:\n",
    "    prec = i[0] / (i[0] + i[1])\n",
    "    precision_conf.append(prec)\n",
    "    rec = i[0] / (i[0] + i[2])\n",
    "    recall_conf.append(rec)\n",
    "    \n",
    "precision_ent = []\n",
    "recall_ent = []\n",
    "for i in threshold_entropy_results:\n",
    "    prec = i[0] / (i[0] + i[1])\n",
    "    precision_ent.append(prec)\n",
    "    rec = i[0] / (i[0] + i[2])\n",
    "    recall_ent.append(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "\n",
    "fig, ax0 = plt.subplots(nrows=1,ncols=1, figsize=(10, 8))\n",
    "\n",
    "ax0.set(title='Precision/Recall - Confidence Threshold', xlabel='Recall', ylabel='Precision')\n",
    "ax0.plot(recall_conf, precision_conf, \"-\")\n",
    "\n",
    "#fig.savefig('thresholds_conf.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "\n",
    "fig, ax0 = plt.subplots(nrows=1,ncols=1, figsize=(10, 8))\n",
    "\n",
    "ax0.set(title='Precision/Recall - Entropy Threshold', xlabel='Recall', ylabel='Precision')\n",
    "ax0.plot(recall_ent, precision_ent, \"-\")\n",
    "\n",
    "#fig.savefig('thresholds_conf.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "\n",
    "fig, ax0 = plt.subplots(nrows=1,ncols=1, figsize=(10, 8))\n",
    "\n",
    "ax0.set(title='Precision/Recall - Confidence Threshold', xlabel='Threshold', ylabel='Count')\n",
    "ax0.plot(thresholds_confidence, precision_conf, \"-\", label=\"%s\" % ('Precision', ))\n",
    "ax0.plot(thresholds_confidence, recall_conf, \"-\", label=\"%s\" % ('Recall', ))\n",
    "ax0.legend(loc=\"upper right\")\n",
    "\n",
    "#fig.savefig('thresholds_conf.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_rate_conf = []\n",
    "false_positive_rate_conf = []\n",
    "for i in threshold_confidence_results:\n",
    "    tpr = i[0] / (i[0] + i[2])\n",
    "    fpr = 1 - tnr\n",
    "    \n",
    "    prec = i[0] / (i[0] + i[1])\n",
    "    precision_conf.append(prec)\n",
    "    rec = i[0] / (i[0] + i[2])\n",
    "    recall_conf.append(rec)\n",
    "    \n",
    "precision_ent = []\n",
    "recall_ent = []\n",
    "for i in threshold_entropy_results:\n",
    "    prec = i[0] / (i[0] + i[1])\n",
    "    precision_ent.append(prec)\n",
    "    rec = i[0] / (i[0] + i[2])\n",
    "    recall_ent.append(rec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
